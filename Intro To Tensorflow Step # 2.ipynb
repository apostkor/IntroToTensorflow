{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Intro To Tensorflow # 2\n",
    "\n",
    "# by SiHyung Park / Sun-il Kim. 2019-01-19\n",
    "---\n",
    "- 잠시 하나만 추가하자면, 전 Step에서 만든 TensorFlow-GPU환경에 모듈을 설치하실때는 \n",
    "- 꼭 터미널에서 가상환경에 들어가신 후 (source/conda activate tf-gpu(가상환경명)) pip이나 conda로 설치해주세요.\n",
    "\n",
    "\n",
    "# TENSORBOARD EXPLAINED IN 5 MIN\n",
    "> loss, accuracy 등 각종 지표들을 손쉽게 시각화 해봅니다\n",
    "\n",
    "![JPEG](https://www.tensorflow.org/images/mnist_tensorboard.png)\n",
    "\n",
    "# TENSORFLOW는 2가지 과정으로 작동됩니다\n",
    "- Operations : 모델을 만들고 수많은 연산장치들을 설정해 둡니다\n",
    "- Tensors : 텐서(데이터)들이 모델속에 설정된 연산장치들을 흘러서 OUTPUT까지 도달합니다\n",
    "\n",
    "#### TENSORBOARD를 통해 작동과정 흐름을 알아볼 수 있습니다\n",
    "\n",
    "# TENSORBOARD의 장점\n",
    "- TENSOR들의 흐름을 파악할 수 있습니다\n",
    "- 디버깅이 가능합니다\n",
    "- 최적화 시키는데 적합합니다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 이전에 했던 코드를 이용해 텐서보드를 작동시켜봅시다\n",
    "\n",
    "import input_data\n",
    "mnist = input_data.read_data_sets('./data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# 학습속도를 나타내는 것으로 너무 높으면 수렴하기전에 날아가버릴 수 있고 (오버슈팅)\n",
    "# 너무 낮으면 최적화 하는데 시간이 너무 오래걸릴 수 있습니다(훈련기간 동안 끝나버림)\n",
    "learning_rate = 0.01\n",
    "# 빨리 시각화 해보기 위해 10번만 훈련합니다\n",
    "training_iteration = 10\n",
    "batch_size = 100\n",
    "display_step = 2\n",
    "\n",
    "# 입력받을 데이터를 담는 '그릇'으로 MNIST 이미지가 가로28, 세로28이라\n",
    "# 둘을 곱한 784를 지정해 줍니다\n",
    "# 정답은 0~9 10가지이기 때문에 OUTPUT은 10으로 정해줍니다\n",
    "X = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float', [None, 10])\n",
    "\n",
    "# 초기에 가중치를 랜덤하게 정해줍니다. 계속 변하는 값이기에 Variable을 써줍니다\n",
    "# placeholder는 변하지 않을 값(input, ouput) variable은 변하는 값을 넣어줍니다\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 텐서보드를 사용해봅니다\n",
    "with tf.name_scope('Wx_b') as scope:\n",
    "    # softmax를 이용한 NeuralNetwork를 사용합니다\n",
    "    model = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "    \n",
    "with tf.name_scope('cost_function') as scope:\n",
    "    # 크로스 엔트로피\n",
    "    cost_function = -tf.reduce_sum(y*tf.log(model))\n",
    "    # cost의 변화를 확인하기 위해 변수를 생성합니다\n",
    "    tf.summary.scalar('cost_function', cost_function)\n",
    "with tf.name_scope('train') as scope:\n",
    "    # Gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "\n",
    "# 나중에 가중치를 확인하기 위한 변수를 생성합니다\n",
    "w_h = tf.summary.histogram('weights', W)\n",
    "b_h = tf.summary.histogram('biases', b)\n",
    "\n",
    "# 시작하기전 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0001 cost= 29.860463738\n",
      "Iteration: 0003 cost= 21.066883641\n",
      "Iteration: 0005 cost= 20.202680029\n",
      "Iteration: 0007 cost= 19.741247816\n",
      "Iteration: 0009 cost= 19.276145857\n",
      "Tuning completed!\n",
      "Accuracy: 0.923\n"
     ]
    }
   ],
   "source": [
    "# 세션을 시작합니다\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # summary들을 저장할 경로를 지정합니다\n",
    "    summary_writer = tf.summary.FileWriter('./data/logs', graph=sess.graph)\n",
    "    \n",
    "    # 위에서 지정한 횟수만큼 훈련해줍니다\n",
    "    for iteration in range(training_iteration):\n",
    "        # cost를 0으로 초기화\n",
    "        avg_cost = 0\n",
    "        # mnist에서 지정한 배치사이즈(한번에 몇칸을 읽을지)로 지정\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # 위에서 지정한 optimizer(GradientDescentOptimizer)를 사용\n",
    "            # input인 x, 정답(label)인 y를 feed_dict에 넣어줍니다 \n",
    "            sess.run(optimizer, feed_dict={X:batch_xs, y:batch_ys})\n",
    "            # loss를 모두 더하고 나중에 평균으로 계산합니다\n",
    "            avg_cost += sess.run(cost_function, feed_dict={X:batch_xs, y:batch_ys})/total_batch\n",
    "            # 매 횟수(epoch라고도 부릅니다)마다 상태를 기록\n",
    "            summary_str = sess.run(merged_summary_op, feed_dict={X: batch_xs, y: batch_ys})\n",
    "            summary_writer.add_summary(summary_str, iteration*total_batch + i)\n",
    "        if iteration % display_step == 0:\n",
    "            print(\"Iteration:\", '%04d' % (iteration + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "        \n",
    "    print('Tuning completed!')\n",
    "    \n",
    "    \n",
    "    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    # 모델의 정확도를 계산하여 출력합니다. \n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, y: mnist.test.labels}))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서보드를 이용해 데이터 시각화를 해봅시다\n",
    "\n",
    "#### JUPYTER NOTEBOOK HOME에서 NEW 탭을 누른 후 TERMINAL을 킵니다\n",
    "- tensorboard --logdir=/home/컴퓨터사용자아이디/data/logs를 입력합니다\n",
    "- /home/이후 경로는 본인이 설정한 경로로 해야합니다\n",
    "- 이는 기본적으로 우분투 기반의 설명입니다.\n",
    "- 윈도우의 경우에도 logs폴더의 경로를 --logdir= 뒤에 두시면 됩니다. \n",
    "\n",
    "#### 위 방법이 안되면 우분투 터미널을 이용해서 할 수도 있습니다\n",
    "- 아나콘다 가상환경을 켠 뒤 \n",
    "- (우분투의 경우 터미널에 source activate tf-gpu(가상환경명) 입력)\n",
    "- (윈도우읭 경우 아나콘다 프롭트에 conda activate tf-gpu(가상환경명) 입력)\n",
    "- tensorboard --logdir=/home/컴퓨터사용자아이디/data/logs\n",
    "- 똑같이 입력해줍니다\n",
    "\n",
    "#   \n",
    "![JPEG](https://i.imgur.com/PLQIHm1.png)\n",
    "\n",
    "#### 드래그 해보면서 훈련결과를 살펴봅니다\n",
    "#   \n",
    "#   \n",
    "#   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW TO MAKE NEURAL NETWORK\n",
    "> 텐서플로우로 CONVOLUTION NEURAL NETWORK를 만들어봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MNIST 데이터를 받아옵니다\n",
    "# one-hot encoding된 상태로 받아옵니다\n",
    "# [house, car, tooth, car]\n",
    "#\n",
    "#\n",
    "# [1,0,0,1], [0,1,0,0], [0,0,1,0], [0,0,0,1] 식으로 바꿔줍니다\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# hyperparameters를 설정합니다\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "# 한번에 128개의 sample을 입력합니다\n",
    "batch_size = 128\n",
    "# 훈련 10번마다 결과를 보기위해 display_step을 지정해줍니다\n",
    "display_step = 10\n",
    "\n",
    "# 28 x 28 image니까 input을 28 x 28 = 784로 지정합니다\n",
    "n_input = 784\n",
    "# 정답이 0~9니까 10개의 정답을 지정해줍니다\n",
    "n_classes = 10\n",
    "# overfitting을 피하기 위해 dropout을 지정해줍니다\n",
    "# 훈련하는 동안 랜덤하게 뉴런을 탈락시켜 한쪽에만 치우쳐 훈련하는것을 방지합니다\n",
    "# 모델을 generalize(regularization)해줍니다\n",
    "# 뉴런의 75%만 사용하고 나머지 25%는 탈락시킵니다\n",
    "dropout = 0.75\n",
    "\n",
    "# input과 output 데이터를 담을 그릇을 지정합니다\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRIDE\n",
    "- 몇 칸씩 옮겨다니면서 사진을 읽을지 지정해줍니다\n",
    "\n",
    "![JPEG](https://camo.githubusercontent.com/3cb9f1b6617911669a2f8265dff7b97235d0b3dc/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a5a436a505546724236654850526934657950366161412e676966)\n",
    "\n",
    "# PADDING\n",
    "- 원본 데이터에 필터를 적용하면 결과값은 전보다 작아집니다\n",
    "- 필터 적용 후 결과 값이 작아지면 처음에 비해 특징이 많이 유실될 수 있습니다\n",
    "- 이를 방지하기 위해 입력값 주위로 0을 넣어서 인위적으로 키웁니다\n",
    "\n",
    "![JPEG](https://t1.daumcdn.net/cfile/tistory/23083C43583ED7621D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN의 layer를 만드는 함수를 생성합니다\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAXPOOLING\n",
    "\n",
    "- M x N의 크기로 잘라낸 후 그안에서 가장 큰 값을 뽑아냅니다\n",
    "- 맥스 풀링은 특징의 값이 큰 값이 다른 특징들을 대표합니다\n",
    "- 전체 데이터의 사이즈가 줄어들기 때문에 컴퓨팅 리소스를 줄이고\n",
    "- 데이터 크기를 줄여가며 손실이 발생하기 때문에 오버피팅을 방지할 수 있습니다\n",
    "![JPEG](https://t1.daumcdn.net/cfile/tistory/2121E641583ED6AF23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxpooling\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 생성합니다\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # input 사이즈를 재조정합니다\n",
    "    # -1의 의미는 앞의 사이즈를 토대로 재조정할 사이즈를 추론하는것입니다\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    # 첫번째 층의 convolution layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    \n",
    "    # 두번째 층의 convolution layerLayer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # 모든 뉴런이 연결된 fully connected layer를 생성해줍니다\n",
    "    # convolution 두개의 층을 지나치면 큰 이미지가 벡터화 됩니다\n",
    "    # 이 벡터들을 가지고 fully connected layer에서 학습(연산)을 해줍니다\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    \n",
    "    # output, predict 과정\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 19315.353516, Training Accuracy= 0.38281\n",
      "Iter 2560, Minibatch Loss= 10591.512695, Training Accuracy= 0.51562\n",
      "Iter 3840, Minibatch Loss= 5397.241211, Training Accuracy= 0.70312\n",
      "Iter 5120, Minibatch Loss= 4887.539062, Training Accuracy= 0.76562\n",
      "Iter 6400, Minibatch Loss= 1506.942871, Training Accuracy= 0.86719\n",
      "Iter 7680, Minibatch Loss= 2708.647705, Training Accuracy= 0.82031\n",
      "Iter 8960, Minibatch Loss= 2554.452148, Training Accuracy= 0.87500\n",
      "Iter 10240, Minibatch Loss= 2522.746582, Training Accuracy= 0.85156\n",
      "Iter 11520, Minibatch Loss= 1672.408936, Training Accuracy= 0.91406\n",
      "Iter 12800, Minibatch Loss= 1826.131836, Training Accuracy= 0.85938\n",
      "Iter 14080, Minibatch Loss= 1781.161743, Training Accuracy= 0.92188\n",
      "Iter 15360, Minibatch Loss= 2800.922852, Training Accuracy= 0.87500\n",
      "Iter 16640, Minibatch Loss= 1930.132080, Training Accuracy= 0.92188\n",
      "Iter 17920, Minibatch Loss= 1205.521729, Training Accuracy= 0.89062\n",
      "Iter 19200, Minibatch Loss= 1602.209717, Training Accuracy= 0.89062\n",
      "Iter 20480, Minibatch Loss= 980.389648, Training Accuracy= 0.94531\n",
      "Iter 21760, Minibatch Loss= 1426.302734, Training Accuracy= 0.86719\n",
      "Iter 23040, Minibatch Loss= 806.575073, Training Accuracy= 0.93750\n",
      "Iter 24320, Minibatch Loss= 1129.600830, Training Accuracy= 0.95312\n",
      "Iter 25600, Minibatch Loss= 388.848358, Training Accuracy= 0.92188\n",
      "Iter 26880, Minibatch Loss= 953.249634, Training Accuracy= 0.92188\n",
      "Iter 28160, Minibatch Loss= 929.984558, Training Accuracy= 0.92188\n",
      "Iter 29440, Minibatch Loss= 1604.772339, Training Accuracy= 0.90625\n",
      "Iter 30720, Minibatch Loss= 484.201782, Training Accuracy= 0.92969\n",
      "Iter 32000, Minibatch Loss= 798.424683, Training Accuracy= 0.92188\n",
      "Iter 33280, Minibatch Loss= 1005.775757, Training Accuracy= 0.93750\n",
      "Iter 34560, Minibatch Loss= 812.050537, Training Accuracy= 0.95312\n",
      "Iter 35840, Minibatch Loss= 689.588379, Training Accuracy= 0.93750\n",
      "Iter 37120, Minibatch Loss= 1068.019653, Training Accuracy= 0.92969\n",
      "Iter 38400, Minibatch Loss= 896.178223, Training Accuracy= 0.92969\n",
      "Iter 39680, Minibatch Loss= 767.992920, Training Accuracy= 0.94531\n",
      "Iter 40960, Minibatch Loss= 521.360229, Training Accuracy= 0.93750\n",
      "Iter 42240, Minibatch Loss= 496.514038, Training Accuracy= 0.93750\n",
      "Iter 43520, Minibatch Loss= 317.251740, Training Accuracy= 0.95312\n",
      "Iter 44800, Minibatch Loss= 1748.861206, Training Accuracy= 0.90625\n",
      "Iter 46080, Minibatch Loss= 1124.532104, Training Accuracy= 0.92188\n",
      "Iter 47360, Minibatch Loss= 479.942200, Training Accuracy= 0.96094\n",
      "Iter 48640, Minibatch Loss= 796.317261, Training Accuracy= 0.94531\n",
      "Iter 49920, Minibatch Loss= 442.014282, Training Accuracy= 0.95312\n",
      "Iter 51200, Minibatch Loss= 611.270874, Training Accuracy= 0.94531\n",
      "Iter 52480, Minibatch Loss= 621.252197, Training Accuracy= 0.94531\n",
      "Iter 53760, Minibatch Loss= 368.494781, Training Accuracy= 0.94531\n",
      "Iter 55040, Minibatch Loss= 780.557129, Training Accuracy= 0.92969\n",
      "Iter 56320, Minibatch Loss= 371.408264, Training Accuracy= 0.96094\n",
      "Iter 57600, Minibatch Loss= 703.759094, Training Accuracy= 0.91406\n",
      "Iter 58880, Minibatch Loss= 360.554382, Training Accuracy= 0.93750\n",
      "Iter 60160, Minibatch Loss= 334.847412, Training Accuracy= 0.96094\n",
      "Iter 61440, Minibatch Loss= 520.330261, Training Accuracy= 0.95312\n",
      "Iter 62720, Minibatch Loss= 209.841110, Training Accuracy= 0.98438\n",
      "Iter 64000, Minibatch Loss= 638.965149, Training Accuracy= 0.93750\n",
      "Iter 65280, Minibatch Loss= 257.702271, Training Accuracy= 0.96875\n",
      "Iter 66560, Minibatch Loss= 274.727051, Training Accuracy= 0.96875\n",
      "Iter 67840, Minibatch Loss= 270.554199, Training Accuracy= 0.96094\n",
      "Iter 69120, Minibatch Loss= 514.106506, Training Accuracy= 0.96094\n",
      "Iter 70400, Minibatch Loss= 730.861755, Training Accuracy= 0.95312\n",
      "Iter 71680, Minibatch Loss= 92.800697, Training Accuracy= 0.97656\n",
      "Iter 72960, Minibatch Loss= 208.665329, Training Accuracy= 0.97656\n",
      "Iter 74240, Minibatch Loss= 163.845169, Training Accuracy= 0.97656\n",
      "Iter 75520, Minibatch Loss= 626.073914, Training Accuracy= 0.94531\n",
      "Iter 76800, Minibatch Loss= 807.136353, Training Accuracy= 0.92969\n",
      "Iter 78080, Minibatch Loss= 351.383362, Training Accuracy= 0.93750\n",
      "Iter 79360, Minibatch Loss= 112.904556, Training Accuracy= 0.98438\n",
      "Iter 80640, Minibatch Loss= 277.747681, Training Accuracy= 0.95312\n",
      "Iter 81920, Minibatch Loss= 543.612915, Training Accuracy= 0.96094\n",
      "Iter 83200, Minibatch Loss= 574.142212, Training Accuracy= 0.96094\n",
      "Iter 84480, Minibatch Loss= 566.590515, Training Accuracy= 0.97656\n",
      "Iter 85760, Minibatch Loss= 463.039154, Training Accuracy= 0.96094\n",
      "Iter 87040, Minibatch Loss= 430.196045, Training Accuracy= 0.96875\n",
      "Iter 88320, Minibatch Loss= 703.446411, Training Accuracy= 0.93750\n",
      "Iter 89600, Minibatch Loss= 126.547493, Training Accuracy= 0.96875\n",
      "Iter 90880, Minibatch Loss= 340.529907, Training Accuracy= 0.96094\n",
      "Iter 92160, Minibatch Loss= 529.671875, Training Accuracy= 0.96875\n",
      "Iter 93440, Minibatch Loss= 61.156197, Training Accuracy= 0.99219\n",
      "Iter 94720, Minibatch Loss= 397.212219, Training Accuracy= 0.96875\n",
      "Iter 96000, Minibatch Loss= 271.023499, Training Accuracy= 0.96094\n",
      "Iter 97280, Minibatch Loss= 344.741089, Training Accuracy= 0.94531\n",
      "Iter 98560, Minibatch Loss= 20.964157, Training Accuracy= 0.99219\n",
      "Iter 99840, Minibatch Loss= 152.502686, Training Accuracy= 0.99219\n",
      "Iter 101120, Minibatch Loss= 370.007996, Training Accuracy= 0.95312\n",
      "Iter 102400, Minibatch Loss= 27.508446, Training Accuracy= 0.98438\n",
      "Iter 103680, Minibatch Loss= 427.035828, Training Accuracy= 0.96875\n",
      "Iter 104960, Minibatch Loss= 297.806732, Training Accuracy= 0.93750\n",
      "Iter 106240, Minibatch Loss= 50.051453, Training Accuracy= 0.99219\n",
      "Iter 107520, Minibatch Loss= 788.365234, Training Accuracy= 0.94531\n",
      "Iter 108800, Minibatch Loss= 332.676117, Training Accuracy= 0.96875\n",
      "Iter 110080, Minibatch Loss= 60.273926, Training Accuracy= 0.98438\n",
      "Iter 111360, Minibatch Loss= 123.056473, Training Accuracy= 0.98438\n",
      "Iter 112640, Minibatch Loss= 361.776611, Training Accuracy= 0.95312\n",
      "Iter 113920, Minibatch Loss= 215.351181, Training Accuracy= 0.96875\n",
      "Iter 115200, Minibatch Loss= 127.722992, Training Accuracy= 0.98438\n",
      "Iter 116480, Minibatch Loss= 128.785812, Training Accuracy= 0.96094\n",
      "Iter 117760, Minibatch Loss= 277.738129, Training Accuracy= 0.97656\n",
      "Iter 119040, Minibatch Loss= 208.048096, Training Accuracy= 0.96875\n",
      "Iter 120320, Minibatch Loss= 16.912689, Training Accuracy= 0.99219\n",
      "Iter 121600, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 122880, Minibatch Loss= 102.903694, Training Accuracy= 0.96875\n",
      "Iter 124160, Minibatch Loss= 134.627350, Training Accuracy= 0.97656\n",
      "Iter 125440, Minibatch Loss= 64.591583, Training Accuracy= 0.99219\n",
      "Iter 126720, Minibatch Loss= 314.896423, Training Accuracy= 0.96875\n",
      "Iter 128000, Minibatch Loss= 423.365814, Training Accuracy= 0.94531\n",
      "Iter 129280, Minibatch Loss= 123.289436, Training Accuracy= 0.97656\n",
      "Iter 130560, Minibatch Loss= 4.165741, Training Accuracy= 0.99219\n",
      "Iter 131840, Minibatch Loss= 8.697006, Training Accuracy= 0.98438\n",
      "Iter 133120, Minibatch Loss= 242.277893, Training Accuracy= 0.96875\n",
      "Iter 134400, Minibatch Loss= 128.898682, Training Accuracy= 0.99219\n",
      "Iter 135680, Minibatch Loss= 121.535149, Training Accuracy= 0.96875\n",
      "Iter 136960, Minibatch Loss= 261.890015, Training Accuracy= 0.96094\n",
      "Iter 138240, Minibatch Loss= 155.238937, Training Accuracy= 0.96094\n",
      "Iter 139520, Minibatch Loss= 174.024765, Training Accuracy= 0.97656\n",
      "Iter 140800, Minibatch Loss= 87.034554, Training Accuracy= 0.97656\n",
      "Iter 142080, Minibatch Loss= 93.698769, Training Accuracy= 0.98438\n",
      "Iter 143360, Minibatch Loss= 91.530563, Training Accuracy= 0.97656\n",
      "Iter 144640, Minibatch Loss= 134.443848, Training Accuracy= 0.96875\n",
      "Iter 145920, Minibatch Loss= 165.365387, Training Accuracy= 0.98438\n",
      "Iter 147200, Minibatch Loss= 79.380043, Training Accuracy= 0.97656\n",
      "Iter 148480, Minibatch Loss= 344.281006, Training Accuracy= 0.96875\n",
      "Iter 149760, Minibatch Loss= 79.969620, Training Accuracy= 0.97656\n",
      "Iter 151040, Minibatch Loss= 322.028259, Training Accuracy= 0.96094\n",
      "Iter 152320, Minibatch Loss= 43.548714, Training Accuracy= 0.99219\n",
      "Iter 153600, Minibatch Loss= 232.779022, Training Accuracy= 0.96094\n",
      "Iter 154880, Minibatch Loss= 62.804771, Training Accuracy= 0.97656\n",
      "Iter 156160, Minibatch Loss= 42.931625, Training Accuracy= 0.97656\n",
      "Iter 157440, Minibatch Loss= 413.289429, Training Accuracy= 0.96094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 158720, Minibatch Loss= 239.798035, Training Accuracy= 0.96094\n",
      "Iter 160000, Minibatch Loss= 182.677704, Training Accuracy= 0.96094\n",
      "Iter 161280, Minibatch Loss= 202.297180, Training Accuracy= 0.96094\n",
      "Iter 162560, Minibatch Loss= 110.513733, Training Accuracy= 0.96875\n",
      "Iter 163840, Minibatch Loss= 36.049911, Training Accuracy= 0.97656\n",
      "Iter 165120, Minibatch Loss= 53.940884, Training Accuracy= 0.98438\n",
      "Iter 166400, Minibatch Loss= 130.659485, Training Accuracy= 0.97656\n",
      "Iter 167680, Minibatch Loss= 240.534073, Training Accuracy= 0.96094\n",
      "Iter 168960, Minibatch Loss= 92.338203, Training Accuracy= 0.98438\n",
      "Iter 170240, Minibatch Loss= 193.878510, Training Accuracy= 0.96094\n",
      "Iter 171520, Minibatch Loss= 87.794662, Training Accuracy= 0.97656\n",
      "Iter 172800, Minibatch Loss= 145.850220, Training Accuracy= 0.97656\n",
      "Iter 174080, Minibatch Loss= 30.708229, Training Accuracy= 0.97656\n",
      "Iter 175360, Minibatch Loss= 165.936127, Training Accuracy= 0.96875\n",
      "Iter 176640, Minibatch Loss= 178.420395, Training Accuracy= 0.99219\n",
      "Iter 177920, Minibatch Loss= 144.749557, Training Accuracy= 0.96094\n",
      "Iter 179200, Minibatch Loss= 64.647240, Training Accuracy= 0.99219\n",
      "Iter 180480, Minibatch Loss= 201.332886, Training Accuracy= 0.97656\n",
      "Iter 181760, Minibatch Loss= 245.480591, Training Accuracy= 0.94531\n",
      "Iter 183040, Minibatch Loss= 132.186157, Training Accuracy= 0.97656\n",
      "Iter 184320, Minibatch Loss= 286.055237, Training Accuracy= 0.96094\n",
      "Iter 185600, Minibatch Loss= 101.705048, Training Accuracy= 0.97656\n",
      "Iter 186880, Minibatch Loss= 290.278503, Training Accuracy= 0.97656\n",
      "Iter 188160, Minibatch Loss= 32.310616, Training Accuracy= 0.98438\n",
      "Iter 189440, Minibatch Loss= 185.680405, Training Accuracy= 0.97656\n",
      "Iter 190720, Minibatch Loss= 60.423286, Training Accuracy= 0.98438\n",
      "Iter 192000, Minibatch Loss= 233.596909, Training Accuracy= 0.96094\n",
      "Iter 193280, Minibatch Loss= 187.365372, Training Accuracy= 0.96875\n",
      "Iter 194560, Minibatch Loss= 33.163506, Training Accuracy= 0.98438\n",
      "Iter 195840, Minibatch Loss= 59.493244, Training Accuracy= 0.99219\n",
      "Iter 197120, Minibatch Loss= 108.186493, Training Accuracy= 0.98438\n",
      "Iter 198400, Minibatch Loss= 72.367439, Training Accuracy= 0.97656\n",
      "Iter 199680, Minibatch Loss= 301.395874, Training Accuracy= 0.97656\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98046875\n"
     ]
    }
   ],
   "source": [
    "# 가중치(weight)를 설정합니다\n",
    "# 딕셔너리를 사용합니다\n",
    "weights = {\n",
    "    # 5x5의 conv, 1개의 input, 32개의 output\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32개의 input, 64개의 output\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024개의 input 10개의 (output)예측결과\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# 모델을 생성합니다\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# optimizer와 loss function을 정의합니다\n",
    "# 손실함수로는 cross_entropy를 사용합니다.\n",
    "# cross_entropy는 정답일 경우 0, 오답일 경우 음의 무한대로 발산합니다\n",
    "# 오류가 클수록 벌을 주는 형식입니다\n",
    "# softmax는 0과 1 사이의 값으로 변환해줍니다 (확률)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# 모델을 평가합니다\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 훈련하기 전, 변수들을 모두 초기화하는 변수를 만듭니다\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# 훈련을 시작합니다\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    \n",
    "    # 맨처음에 정한 훈련횟수만큼 훈련합니다\n",
    "    while step * batch_size < training_iters:\n",
    "        # batch size를 불러옵니다\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # backpropagation을 합니다\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       keep_prob: dropout})\n",
    "        # 훈련 10번마다 결과를 확인합니다\n",
    "        if step % display_step == 0:\n",
    "            # 오차와 정확도를 측정합니다\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # 총 결과를 출력합니다\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
