{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro To Tensorflow Step #1\n",
    "==\n",
    "by Sun-il Kim, 2019-01-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적인 수학과 알고리즘에 대한 이해가 잡혔으니,\n",
    "\n",
    "이제 실제로 유명 오픈 소스 라이브러리들을 사용해서 여러 실무급 예제를 실습해보겠습니다. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://cdn-images-1.medium.com/max/1400/1*eNd_RaITEus4c0dP2Ty2gw.jpeg \"Title Text\")\n",
    "\n",
    "일반적으로 **Scikit-Learn 과 Tensorflow**를 가장 많이 사용하게 됩니다. \n",
    "\n",
    "\n",
    "(물론 계산을 위한 numpy, 데이터프레임을 위한 pandas, 그래프를 위한 matplotlib은 덤입니다.)\n",
    "\n",
    "\n",
    "먼저 Scikit-Learn 입니다. \n",
    "--\n",
    "\n",
    "![alt text 2](http://s5047.pcdn.co/wp-content/uploads/2015/04/drop_shadows_background2-1024x563.png \"Title Text\")\n",
    "\n",
    "\n",
    "이젠 아는 이름이 많이 보입니다. \n",
    "- Clasification(분류)의 SVM, Naive Bayes\n",
    "- Clustering(군집화)의 K-Means\n",
    "- Regression(회귀)의 Linear Regression, Random Forest(주로 분류지만)\n",
    "- Dimensionality Reduction(차원축소)의 PCA, Kernal approx\n",
    "\n",
    "**등등 sklearn(Scikit-Learn) 라이브러리 안에 거의 모든 기계학습 알고리즘들이 들어있음을 알 수 있습니다.**\n",
    "\n",
    "\n",
    "\n",
    "#   \n",
    "앞서 Math of Intelligence에서는 하나하나 직접 코딩을 했었습니다. \n",
    "\n",
    "그럼에도 Python 특성상 엄청 코드가 길지는 않았습니다만, 라이브러리를 사용하면 '조금' 쉬워집니다. \n",
    "\n",
    "\n",
    "예를 들어 Random Forest를 구현하여 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,Y 데이터가 정리되어있다는 가정 하에, \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이게 전부입니다. \n",
    "\n",
    "이번에는 SVM을 구연해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역시 X,Y 데이터가 정리되어있다는 가정 하에,\n",
    "from sklearn.svm import SVR\n",
    "svm = SVR(kernel='linear', C=1e3)\n",
    "svm.fit(X_train_scaled, y_train.values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참 쉽지요? \n",
    "\n",
    "그래도 알고리즘 자체를 이해하지 못하면, \n",
    "\n",
    "    - Hyper Parameter들이 무슨 동작을 하는지, 무슨 의미를 가지는지 알 수 없고\n",
    "    - 데이터를 어떻게 처리해야 하는지 모르며\n",
    "    - 특정 데이터엔 어떠한 알고리즘을 사용해야 하는지 알 수 없기 때문에 \n",
    "\n",
    "때문에 지금까지의 학습이 있었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**다만 Scikit-Learn 에도 단점은 존재합니다.**\n",
    "\n",
    "\n",
    "먼저,\n",
    "- 홀로는 GPU연산을 할 수 없어 속도에서 많이 떨어지며\n",
    "- 시각화가 matplotlib등 기타 모듈이 도움이 없으면 불가능하고\n",
    "- 뉴럴 네트워크 구현을 지원하지 않습니다. (최근에 추가되었단 소리도 있습니다만)\n",
    "\n",
    "때문에 쉽고 빠른 testing을 위해 많이 사용하는 편 입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 Tensorflow\n",
    "==\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래는 Tensorflow 와 Theano 라는 라이브러리의 경쟁구도였습니다.\n",
    "\n",
    "![alt text 2](https://www.analyticsindiamag.com/wp-content/uploads/2017/12/Untitled-2-01.jpg \"Title Text\")\n",
    "\n",
    "- Theano는 Window OS를 처음부터 지원했으며, 1개의 GPU구조에선 속도도 더 유리했습니다. \n",
    "- 더 low-level의 코딩이 가능하여 더 세밀한 구현이 가능하다는 이야기가 있었습니다만..(c를 안쓰는 이유)\n",
    "\n",
    "그러나 Tensorflow가 \n",
    "- 점점 더 많은 커뮤니티의 참여로 Documentation이나 Github등의 자료양이 월등하고\n",
    "- '텐서보드(Tensorboard)' 등의 방법으로 시각화가 되며 (아주이쁨)\n",
    "- 강화학습 (Reinforcement learning)등 알고리즘 자체가 더 다양하며 \n",
    "- 멀티 GPU지원을 하는 것은 덤, 이제는 어떠한 환경에서도 더 빠른 모습을 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Linear Gradient Descent 알고리즘을 Tensorflow로 구현하여 보겠습니다.**\n",
    "==\n",
    "\n",
    "\n",
    "\n",
    "먼저 아나콘다+주피터노트북 기준 Tensorflow 설치를 진행합니다. \n",
    "\n",
    "\n",
    "**$ conda create --name tf-gpu python=3.6**  \n",
    "\n",
    "*tf-gpu라는 아나콘다 가상환경을 만들고, $ source activate tf-gpu로 가상환경에 들어갑니다 (Windows는 source가 아닌 conda)*\n",
    "\n",
    "**$ conda install tensorflow-gpu** \n",
    "\n",
    "*GPU버전의 Tensorflow를 설치합니다. (CPU버전은 -gpu를 제외하시면 됩니다.)*\n",
    "\n",
    "**$ conda install ipykernal**\n",
    "\n",
    "**$ python -m ipykernel install --user --name tf-gpu --display-name \"TensorFlow-GPU\"**\n",
    "\n",
    "*이제부터는 주피터노트북에서 파일을 생성 시, \"Tensorflow-GPU\"로 생성합니다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.7439022  -0.19278115]] [0.42352206]\n",
      "20 [[0.20549454 0.10982043]] [0.29669476]\n",
      "40 [[0.11947503 0.18277028]] [0.29971543]\n",
      "60 [[0.10356836 0.1966742 ]] [0.3000424]\n",
      "80 [[0.10064603 0.19934854]] [0.3000353]\n",
      "100 [[0.10011469 0.19986975]] [0.30001447]\n",
      "120 [[0.10001969 0.19997324]] [0.30000493]\n",
      "140 [[0.10000317 0.19999433]] [0.30000156]\n",
      "160 [[0.10000046 0.19999877]] [0.30000046]\n",
      "180 [[0.10000004 0.1999997 ]] [0.30000013]\n",
      "200 [[0.1        0.19999988]] [0.30000007]\n"
     ]
    }
   ],
   "source": [
    "# 코딩을 해봅시다!\n",
    "\n",
    "# 드디어 텐서플로우 라이브러리를 가져옵니다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Numpy로 랜덤한 데이터를 만들어 저장합니다. \n",
    "x_data = np.float32(np.random.rand(2, 100)) # 랜덤 입력\n",
    "y_data = np.dot([0.100, 0.200], x_data) + 0.300\n",
    "\n",
    "# 선형 모델을 만듦니다. y=mx+b.\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))\n",
    "y = tf.matmul(W, x_data) + b\n",
    "\n",
    "# GradientDescentOptimizer 모듈을 불러와 최적화를 진행합니다. \n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "\n",
    "# 직관적으로 loss를 minimize 하는 모듈을 불러옵니다. \n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 모든 변수를 초기화하는 모듈을 불러옵니다. \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 세션을 생성합니다. \n",
    "sess = tf.Session()\n",
    "# 세션을 초기화합니다.\n",
    "sess.run(init)\n",
    "\n",
    "# 세션을 학습합니다. \n",
    "for step in range(0, 201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print (step, sess.run(W), sess.run(b))\n",
    "\n",
    "# W: [[0.100  0.200]], b: [0.300] 에 모델이 수렴함을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**물론 Scikit-Learn과 Tensorflow를 제외하고도 수많은 라이브러리들이 있습니다.**\n",
    "\n",
    "- 이제는 쇠퇴한(?) \"Caffe\"\n",
    "- \"TF Learn\" or \"TF Keras\", Tensorflow에 최적화된 Keras 라이브러리 \n",
    "- \"Keras\"는 Tensorflow위에 만들어진 딥러닝용 High level API로 더 간단한 장점을 가지고 있습니다.\n",
    "\n",
    "그래도 현재까지도 \n",
    "\n",
    "- 연구를 위한 라이브러리는 => Tensorflow\n",
    "- 상용화를 위한 라이브러리는 => Tensorflow\n",
    "- 초보자를 위한 라이브러리는 => TF Keras \n",
    "\n",
    "로 통용되고 있습니다. 때문에 저희는 Tensorflow를 주로 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "금일 마지막 단계로 Tensorflow로 MNIST(숫자인식)을 진행해보겠습니다. \n",
    "==\n",
    "\n",
    "먼저 MNIST 데이터를 받아오기 위해 간단한 파이썬 코드를 하나 받아(input_data.py), 작업하고 있는 노트북의 위치에 저장해둡니다. \n",
    "\n",
    "\n",
    "https://drive.google.com/open?id=1AI963w5hfc7ZCugQ8kMpw3_h9eWPu6GI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'input_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-368f4433b051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'input_data'"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
