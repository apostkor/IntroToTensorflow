{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro To Tensorflow Step #Intro\n",
    "==\n",
    "by Sun-il Kim, 2019-01-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적인 수학과 알고리즘에 대한 이해가 잡혔으니,\n",
    "\n",
    "이제 실제로 유명 오픈 소스 라이브러리들을 사용해서 여러 실무급 예제를 실습해보겠습니다. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://cdn-images-1.medium.com/max/1400/1*eNd_RaITEus4c0dP2Ty2gw.jpeg \"Title Text\")\n",
    "\n",
    "일반적으로 **Scikit-Learn 과 Tensorflow**를 가장 많이 사용하게 됩니다. \n",
    "\n",
    "\n",
    "(물론 계산을 위한 numpy, 데이터프레임을 위한 pandas, 그래프를 위한 matplotlib은 덤입니다.)\n",
    "\n",
    "\n",
    "먼저 Scikit-Learn 입니다. \n",
    "--\n",
    "\n",
    "![alt text 2](http://s5047.pcdn.co/wp-content/uploads/2015/04/drop_shadows_background2-1024x563.png \"Title Text\")\n",
    "\n",
    "\n",
    "이젠 아는 이름이 많이 보입니다. \n",
    "- Clasification(분류)의 SVM, Naive Bayes\n",
    "- Clustering(군집화)의 K-Means\n",
    "- Regression(회귀)의 Linear Regression, Random Forest(주로 분류지만)\n",
    "- Dimensionality Reduction(차원축소)의 PCA, Kernal approx\n",
    "\n",
    "**등등 sklearn(Scikit-Learn) 라이브러리 안에 거의 모든 기계학습 알고리즘들이 들어있음을 알 수 있습니다.**\n",
    "\n",
    "\n",
    "\n",
    "#   \n",
    "앞서 Math of Intelligence에서는 하나하나 직접 코딩을 했었습니다. \n",
    "\n",
    "그럼에도 Python 특성상 엄청 코드가 길지는 않았습니다만, 라이브러리를 사용하면 '조금' 쉬워집니다. \n",
    "\n",
    "\n",
    "예를 들어 Random Forest를 구현하여 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,Y 데이터가 정리되어있다는 가정 하에, \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이게 전부입니다. \n",
    "\n",
    "이번에는 SVM을 구연해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역시 X,Y 데이터가 정리되어있다는 가정 하에,\n",
    "from sklearn.svm import SVR\n",
    "svm = SVR(kernel='linear', C=1e3)\n",
    "svm.fit(X_train_scaled, y_train.values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참 쉽죠? \n",
    "\n",
    "그래도 알고리즘 자체를 이해하지 못하면, \n",
    "\n",
    "    - Hyper Parameter들이 무슨 동작을 하는지, 무슨 의미를 가지는지 알 수 없고\n",
    "    - 데이터를 어떻게 처리해야 하는지 모르며\n",
    "    - 특정 데이터엔 어떠한 알고리즘을 사용해야 하는지 알 수 없기 때문에 \n",
    "\n",
    "지금까지의 학습이 있었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**다만 Scikit-Learn 에도 단점은 존재합니다.**\n",
    "\n",
    "\n",
    "먼저,\n",
    "- 홀로는 GPU연산을 할 수 없어 속도에서 많이 떨어지며\n",
    "- 시각화가 matplotlib등 기타 모듈이 도움이 없으면 불가능하고\n",
    "- 뉴럴 네트워크 구현을 지원하지 않습니다. (최근에 추가되었단 소리도 있습니다만)\n",
    "\n",
    "때문에 쉽고 빠른 testing을 위해 많이 사용하는 편 입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 Tensorflow\n",
    "==\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래는 Tensorflow 와 Theano 라는 라이브러리의 경쟁구도였습니다.\n",
    "\n",
    "![alt text 2](https://www.analyticsindiamag.com/wp-content/uploads/2017/12/Untitled-2-01.jpg \"Title Text\")\n",
    "\n",
    "- Theano는 Window OS를 처음부터 지원했으며, 1개의 GPU구조에선 속도도 더 유리했습니다. \n",
    "- 더 low-level의 코딩이 가능하여 더 세밀한 구현이 가능하다는 이야기가 있었습니다만..(c를 안쓰는 이유)\n",
    "\n",
    "그러나 Tensorflow가 \n",
    "- 점점 더 많은 커뮤니티의 참여로 Documentation이나 Github등의 자료양이 월등하고\n",
    "- '텐서보드(Tensorboard)' 등의 방법으로 시각화가 되며 (아주이쁨)\n",
    "- 강화학습 (Reinforcement learning)등 알고리즘 자체가 더 다양하며 \n",
    "- 멀티 GPU지원을 하는 것은 덤, 이제는 어떠한 환경에서도 더 빠른 모습을 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Gradient Descent 알고리즘을 Tensorflow로 구현하여 보겠습니다.**\n",
    "==\n",
    "\n",
    "\n",
    "\n",
    "먼저 아나콘다+주피터노트북 기준 Tensorflow 설치를 진행합니다. \n",
    "\n",
    "\n",
    "**$ conda create --name tf-gpu python=3.6**  \n",
    "\n",
    "*tf-gpu라는 아나콘다 가상환경을 만들고, $ source activate tf-gpu로 가상환경에 들어갑니다 (Windows는 source가 아닌 conda)*\n",
    "\n",
    "**$ conda install tensorflow-gpu** \n",
    "\n",
    "*GPU버전의 Tensorflow를 설치합니다. (CPU버전은 -gpu를 제외하시면 됩니다.)*\n",
    "\n",
    "**$ conda install ipykernal**\n",
    "\n",
    "**$ python -m ipykernel install --user --name tf-gpu --display-name \"TensorFlow-GPU\"**\n",
    "\n",
    "*이제부터는 주피터노트북에서 파일을 생성 시, \"Tensorflow-GPU\"로 생성합니다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.7439022  -0.19278115]] [0.42352206]\n",
      "20 [[0.20549454 0.10982043]] [0.29669476]\n",
      "40 [[0.11947503 0.18277028]] [0.29971543]\n",
      "60 [[0.10356836 0.1966742 ]] [0.3000424]\n",
      "80 [[0.10064603 0.19934854]] [0.3000353]\n",
      "100 [[0.10011469 0.19986975]] [0.30001447]\n",
      "120 [[0.10001969 0.19997324]] [0.30000493]\n",
      "140 [[0.10000317 0.19999433]] [0.30000156]\n",
      "160 [[0.10000046 0.19999877]] [0.30000046]\n",
      "180 [[0.10000004 0.1999997 ]] [0.30000013]\n",
      "200 [[0.1        0.19999988]] [0.30000007]\n"
     ]
    }
   ],
   "source": [
    "# 코딩을 해봅시다!\n",
    "\n",
    "# 드디어 텐서플로우 라이브러리를 가져옵니다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Numpy로 랜덤한 데이터를 만들어 저장합니다. \n",
    "x_data = np.float32(np.random.rand(2, 100)) # 랜덤 입력\n",
    "y_data = np.dot([0.100, 0.200], x_data) + 0.300\n",
    "\n",
    "# 선형 모델을 만듦니다. y=mx+b.\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))\n",
    "y = tf.matmul(W, x_data) + b\n",
    "\n",
    "# GradientDescentOptimizer 모듈을 불러와 최적화를 진행합니다. \n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "\n",
    "# 직관적으로 loss를 minimize 하는 모듈을 불러옵니다. \n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 모든 변수를 초기화하는 모듈을 불러옵니다. \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 세션을 생성합니다. \n",
    "sess = tf.Session()\n",
    "# 세션을 초기화합니다.\n",
    "sess.run(init)\n",
    "\n",
    "# 세션을 학습합니다. \n",
    "for step in range(0, 201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print (step, sess.run(W), sess.run(b))\n",
    "\n",
    "# W: [[0.100  0.200]], b: [0.300] 에 모델이 수렴함을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**물론 Scikit-Learn과 Tensorflow를 제외하고도 수많은 라이브러리들이 있습니다.**\n",
    "\n",
    "- 이제는 쇠퇴한(?) \"Caffe\"\n",
    "- \"TF Learn\" or \"TF Keras\", Tensorflow에 최적화된 Keras 라이브러리 \n",
    "- \"Keras\"는 Tensorflow위에 만들어진 딥러닝용 High level API로 더 간단한 장점을 가지고 있습니다.\n",
    "\n",
    "그래도 현재까지도 \n",
    "\n",
    "- 연구를 위한 라이브러리는 => Tensorflow\n",
    "- 상용화를 위한 라이브러리는 => Tensorflow\n",
    "- 초보자를 위한 라이브러리는 => TF Keras \n",
    "\n",
    "로 통용되고 있습니다. 때문에 저희는 Tensorflow를 주로 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "금일 마지막 단계로 Tensorflow로 MNIST(숫자인식)을 진행해보겠습니다. \n",
    "==\n",
    "\n",
    "먼저 MNIST 데이터를 받아오기 위해 간단한 파이썬 코드를 하나 받아(input_data.py), 작업하고 있는 노트북의 위치에 저장해둡니다. \n",
    "\n",
    "\n",
    "https://drive.google.com/open?id=1AI963w5hfc7ZCugQ8kMpw3_h9eWPu6GI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 위에 다운받은 input_data.py 파일을 불러와 현재위치에 데이터를 저장합니다. \n",
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"./data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우를 불러옵니다. \n",
    "import tensorflow as tf\n",
    "\n",
    "# 하이퍼파라미터들을 설정합니다. \n",
    "\n",
    "# 학습속도를 설정합니다. 0.01이란 학습속도는 MNIST분석에서 효율적이라고 알려져있는 학습속도입니다. \n",
    "# 학습속도에 따라 가중치가 얼마나 빨리 업데이트될지 정해집니다. \n",
    "# 학습속도가 너무 빠르면, 가장 최적화된 지점을 넘겨버릴 수 도 있고\n",
    "# 학습속도가 너무 느리면, 최적화되기까지 너무 오래걸리거나 평생 계산할 수 도 있습니다. \n",
    "# 때문에 다른 문제를 해결할 시, 하이퍼파라미터 최적화를 따로 해야합니다. \n",
    "learning_rate = 0.01\n",
    "# 반복횟수를 설정합니다. \n",
    "training_iteration = 30\n",
    "batch_size = 100\n",
    "display_step = 2\n",
    "\n",
    "# \"텐서플로우\"이기 때문에 입력도 텐서, 출력도 텐서입니다. \n",
    "\n",
    "# TF graph input\n",
    "# \"플레이스홀더\"란 나중에 데이터를 넣을 공간이며\n",
    "# 현재는 초기화하지도 데이터를 넣지도 않을 변수입니다. \n",
    "# \"플레이스홀더\"에 나중에 입력될 데이터의 구조를 파라미터로 넣는 것인데, \n",
    "# MNIST 데이터 이미지는 28*28의 백터구조인데, 이를 28*28=784의 1차원텐서로 바꿔놓을 것이기 때문에 784입니다. \n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "# 결과(y)의 \"플레이스홀더\"는 당연히 0~9까지 10가지의 결과가 나올 수 있기 때문에 10입니다. \n",
    "y = tf.placeholder(\"float\", [None, 10]) \n",
    "\n",
    "# Create a model\n",
    "\n",
    "# 모델의 초기 가중치를 정합니다. 이는 계속 업데이트되어 최적화되게 됩니다. \n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# \"스코프\"는 나중에 \"텐서보드\"에서 시각화를 할 수 있게 하기 위해 만드는 것입니다. \n",
    "# \"그래프\"를 생성한다고 생각하변 됩니다. 나중에 탠서보드를 보시면 이해가 되실겁니다. \n",
    "with tf.name_scope(\"Wx_b\") as scope:\n",
    "    # Linear 모델을 만듦니다.\n",
    "    model = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "    # Cost는 에러를 최소화하기 위해 생성합니다.\n",
    "with tf.name_scope(\"cost_function\") as scope:\n",
    "    # \"크로스 엔트로피\"를 사용하여 에러를 최소화합니다.\n",
    "    cost_function = -tf.reduce_sum(y*tf.log(model))\n",
    "    # Cost가 어떻게 변화하고 있는지 확인하기 위해 변수를 생성합니다. \n",
    "    tf.summary.scalar(\"cost_function\", cost_function)\n",
    "    # Train 스코프도 만듦니다. \n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    # Gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "    \n",
    "# \"서머리\"는 나중에 가중치등의 변화를 확인하기 위해 만드는 변수입니다. \n",
    "w_h = tf.summary.histogram(\"weights\", W)\n",
    "b_h = tf.summary.histogram(\"biases\", b)\n",
    "\n",
    "\n",
    "\n",
    "# 모든 변수 초기화를 진행하고,\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 귀찮기 때문에 모든 서머리들을 도합합니다. \n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0001 cost= 29.860463738\n",
      "Iteration: 0003 cost= 21.061188916\n",
      "Iteration: 0005 cost= 20.130096987\n",
      "Iteration: 0007 cost= 19.675449629\n",
      "Iteration: 0009 cost= 19.291439551\n",
      "Iteration: 0011 cost= 19.142024592\n",
      "Iteration: 0013 cost= 18.981028086\n",
      "Iteration: 0015 cost= 18.816097370\n",
      "Iteration: 0017 cost= 18.680070918\n",
      "Iteration: 0019 cost= 18.652365285\n",
      "Iteration: 0021 cost= 18.414752931\n",
      "Iteration: 0023 cost= 18.279202321\n",
      "Iteration: 0025 cost= 18.196379765\n",
      "Iteration: 0027 cost= 18.115819153\n",
      "Iteration: 0029 cost= 18.135900959\n",
      "Tuning completed!\n",
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "# 세션을 초기화하고 실행합니다. (그래프를 생성합니다.)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # 모든 서머리들이 저장될 위치를 정합니다.\n",
    "    # 나중에 이 경로로 텐서보드를 실행시켜야 하기 때문에, 꼭 바꾸고 기억해줍시다.\n",
    "    summary_writer = tf.summary.FileWriter('./data/logs', graph=sess.graph)\n",
    "\n",
    "    # 드디어 지금까지 준비한 데이터와 스코프로 트레이닝을 시작합니다. \n",
    "    for iteration in range(training_iteration):\n",
    "        avg_cost = 0. # Cost를 초기화합니다. \n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # 데이터 크기를 확인하고 트레이닝 시작합니다.\n",
    "        # 확인된 데이터 사이즈만큼 반복하여 학습합니다. \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # 데이터를 기반으로 트레이닝합니다. \n",
    "            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # 평균 Loss를 계산합니다. \n",
    "            avg_cost += sess.run(cost_function, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "            # 매 회차마다 현황을 기록합니다.\n",
    "            summary_str = sess.run(merged_summary_op, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            summary_writer.add_summary(summary_str, iteration*total_batch + i)\n",
    "        # 매 회차마다 현황을 출력합니다.\n",
    "        if iteration % display_step == 0:\n",
    "            print(\"Iteration:\", '%04d' % (iteration + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Tuning completed!\")\n",
    "\n",
    "    # 모델을 test합니다.\n",
    "    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    # 모델의 정확도를 계산하여 출력합니다. \n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**결과를 \"텐서보드\"로 확인해 보겠습니다.**\n",
    "\n",
    "1. 노트북 파일 생성시 New-Terminal을 누르면 주피터노트북 안에서도 터미널이 켜집니다. \n",
    "2. tensorboard --logdir=/home/apostsik/Projects/git/IntroToTensorflow/data/logs (자신의 logs 폴더위치까지)\n",
    "3. 그렇게 입력하면, 포트번호와 함께 주소가 출력됩니다.\n",
    "4. 주소는 권한제한으로 안들어가지겠지만, 인터넷창에 \"localhost:포트번호\" 입력하시면 텐서보드가 뜨게됩니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text 1](https://i.imgur.com/dN3DLkf.png \"Title Text\")\n",
    "\n",
    "Cost_function이 스칼라값의 변동이었고 이가 표시됩니다. \n",
    "\n",
    "![alt text 1](https://i.imgur.com/k4BC0UA.png \"Title Text\")\n",
    "\n",
    "이번에는 아까 생성한 \"스코프\"들을 확인합니다. \n",
    "\n",
    "이것으로 어떻게 데이터가 움직이고 있는지 시각적으로 확인할 수 있습니다.\n",
    "\n",
    "![alt text 1](https://i.imgur.com/4qeTiwK.png \"Title Text\")\n",
    "\n",
    "그 외의 정보들 역시 시각적으로 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
