{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Intro To Tensorflow # 5\n",
    "\n",
    "# by PARK-SI HYUNG. 2019-01-23\n",
    "---\n",
    "\n",
    "# Q-LEARNING FOR TRADING\n",
    "> 강화학습을 이용해 주식투자를 해봅니다\n",
    "\n",
    "![JPEG](http://manul.io/img/gekkos/arch.png)\n",
    "\n",
    "\n",
    "#   \n",
    "# MARKOV DECISION PROCESS\n",
    "> 다음 상태의 확률은 오직 현재의 상태와 현재의 행동에만 영향을 받습니다\n",
    "\n",
    "#### Markov Decision Process는 강화학습의 기반을 이루고 있습니다\n",
    "- 한 STATE가 다른 STATE 이동할 확률의 합은 1을 유지한 상태로\n",
    "- 여러 STATE가 연쇄적으로 이어져 있습니다\n",
    "- 여기에 REWARD를 추가하여 다음 STATE 가는 것이 얼마나 가치있는지 알 수 있습니다\n",
    "\n",
    "![JPEG](https://t1.daumcdn.net/cfile/tistory/99904A4C5A36185431)\n",
    "\n",
    "# HOW MARKOV DECISION PROCESS WORKS?\n",
    "- 1. T 시점에 STATE S에 놓인 AGENT가 POLICY에 따라 ACTION A를 수행합니다\n",
    "- 2. STATE S에서 ACTION A를 수행하면 REWARD를 받습니다\n",
    "- 3. TRANSITION PROBABILITY에 따라 STATE S'로 이동\n",
    "\n",
    "![JPEG](https://i.imgur.com/xtxTgO1.png)\n",
    "\n",
    "#### POLICY : POLICY : STATE S에서 ACTOIN A를 취할 확률\n",
    "#### TRANSITION PROBABILITY : STATE S에서 ACTION A를 해서 STATE S'로 이동할 확률\n",
    "\n",
    "# STATE\n",
    "- AGENT가 인식하는 자신의 상태입니다\n",
    "- 주식의 경우 우리가 보유하고 있는 주식의 양, 주식의 시장 가격 등이 되겠습니다\n",
    "- ARRAY로 표현이 가능합니다\n",
    "#   \n",
    "\n",
    "# ACTION\n",
    "- 주식시장에서는 BUY, SELL, HOLD의 세가지 행동만 존재합니다\n",
    "- AGENT가 ACTION을 취함에 따라 STATE를 변화시킬 수 있습니다\n",
    "#   \n",
    "\n",
    "# AGENT\n",
    "- 행동을 하는 주체\n",
    "#   \n",
    "\n",
    "# ENVIRONMENT\n",
    "- AGENT가 행동을 취하는 공간(게임이라면 맵)\n",
    "#      \n",
    "\n",
    "# REWARDS\n",
    "- AGENT가 ACTION을 취하면 그에 따른 REWARD를 'ENVIRONMENT'가 AGENT에게 알려줍니다\n",
    "- 바둑의 경우는 승패, 주식의 경우는 얼마나 오를지 내릴지가 되겠습니다\n",
    "![JPEG](http://www.modulabs.co.kr/files/attach/images/334/192/003/af927db4928fa1c9c68c133ea73e0737.png)\n",
    "\n",
    "\n",
    "#   \n",
    "\n",
    "---\n",
    "\n",
    "# DISCOUNTED FUTURE REWARD\n",
    "> 할인된 미래의 보상(!?)\n",
    "\n",
    "- 오랜 시간동안 좋은 수행능력이 나오기 위해선 당장의 REWARD를 포함해서 미래에 얻을\n",
    "- REWARD도 반영할 수 있어야 합니다\n",
    "- AGENT에 가장 좋은 지침은 미래의 REWARD를 극대화시켜서 행동을 택하게 하는것 입니다\n",
    "- 보통 현재의 보상에 가중치를 높게 주고 미래의 보상에 가중치를 낮게 줍니다\n",
    "#   \n",
    "#  Q-LEARNING\n",
    "> MARKOV DECISION PROCESS와 다르게 모델을 몰라도 학습하는 방법(ACTION VALUE)\n",
    "\n",
    "- MARKOV DECISION PROCESS는 다음 STATE들에 대한 정보를 모두 알아야 하고\n",
    "- 그 STATE로 가려면 어떻게 해야하는 지도 알아야 합니다\n",
    "- 이렇게, 다음 STATE의 정보와 상관없이 ACTION으로만 평가하는것이 Q-LEARNING 입니다\n",
    "\n",
    "![JPEG](http://www.modulabs.co.kr/files/attach/images/334/237/003/90c80cf356a95548c5fac0702e528280.png)\n",
    "\n",
    "- Q-FUNCTION은 '주어진 상태에서 행동을 수행한 후 게임이 끝날 때 가능한 최상의 점수'입니다\n",
    "- Q-FUNCTION을 구하기 위해 BELLMAN 방정식을 반복적으로 사용하여 계산합니다\n",
    "\n",
    "#   \n",
    "# BELLMAN EQUATION\n",
    "> ACTION VALUE FUNCTION\n",
    "\n",
    "- 현재 상태와 행동에 대한 미래의 최대 보상은, 즉각적인 보상과 다음 상태에서 얻을 수 있는<BR>미래의 최대 보상의 합 입니다\n",
    "- 모든 경로를 처음부터 따라가지 않고, SAMPLING을 통해서 '한 번 가보자' 생각하고 실행한 <BR>후에 학습합니다 \n",
    "    \n",
    "![JPEG](https://i.imgur.com/IK7oFYV.png)\n",
    "#   \n",
    "\n",
    "# HOW Q-LEARNING WORKS ?\n",
    "\n",
    "- Q(s,a)를 초기화합니다. \n",
    "- ACTION A를 선택합니다\n",
    "- ACTION A를 수행 해서 S'으로 넘어갑니다.\n",
    "- S'으로 넘어갈 때의 REWARD를 측정합니다\n",
    "- Q를 UPDATE하고 다시 s로 돌아가 최적의 Q를 찾을때까지 반복합니다\n",
    "\n",
    "#     \n",
    "\n",
    "![JPEG](https://i.imgur.com/zYJ1WW3.png)\n",
    "\n",
    "#    \n",
    "#     \n",
    "#     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-LEARNING TUTORIAL\n",
    "#   \n",
    "#### https://github.com/llSourcell/Reinforcement_Learning_for_Stock_Prediction\n",
    "- 위 사이트에서 Clone or download로 파일을 모두 받고\n",
    "- 터미널을 이용해 다운받은 폴더까지 이동한 다음\n",
    "\n",
    "#### mkdir model\n",
    "#### python train.py ^GSPC 10 10\n",
    "\n",
    "- 을 입력해 훈련시킵니다.\n",
    "- GSPC는 S&P500 식별코드고 앞의 숫자는 WINDOW(한번에 입력받는 양)\n",
    "- 뒤의 숫자는 훈련 횟수입니다\n",
    "---\n",
    "#### python evaluate.py ^GSPC_2011 model_ep10\n",
    "- 훈련이 끝나면 위 코드를 입력해 평가해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py 코드를 살펴봅시다\n",
    "\n",
    "# Agent.py에서 Agent를 불러옵니다\n",
    "from agent.agent import Agent\n",
    "from functions import *\n",
    "import sys\n",
    "\n",
    "# 입력으로 주식, 한번에 입력받는 수, 훈련횟수를 입력합니다\n",
    "# 하나라도 빠지면 훈련이 시작되지 않게합니다\n",
    "if len(sys.argv) != 4:\n",
    "    print(\"Usage: python train.py [stock] [window] [episodes]\")\n",
    "    exit()\n",
    "\n",
    "# 입력받은 인자들을 stock_name, window_size, episode_count에 넣어줍니다\n",
    "stock_name, window_size, episode_count = sys.argv[1], int(sys.argv[2]), int(sys.argv[3])\n",
    " \n",
    "agent = Agent(window_size)\n",
    "data = getStockDataVec(stock_name)\n",
    "l = len(data) - 1\n",
    "batch_size = 32\n",
    "\n",
    "# 훈련횟수만큼 훈련시킵니다\n",
    "for e in range(episode_count + 1):\n",
    "    print(\"Episode \" + str(e) + \"/\" + str(episode_count))\n",
    "    # state를 초기화 해줍니다\n",
    "    state = getState(data, 0, window_size + 1)\n",
    "\n",
    "    total_profit = 0\n",
    "    agent.inventory = []\n",
    "    \n",
    "    # data의 수만큼 훈련시킵니다\n",
    "    for t in range(l):\n",
    "        # agent가 action을 합니다\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # 사거나 팔지않을 경우 hold합니다\n",
    "        # 다음 state로 넘어가고 reward는 0\n",
    "        next_state = getState(data, t + 1, window_size + 1)\n",
    "        reward = 0\n",
    "        \n",
    "        # action==1이면 주식을 삽니다\n",
    "        # action은 agent가 결정합니다\n",
    "        if action == 1: # buy\n",
    "            agent.inventory.append(data[t])\n",
    "            print(\"Buy: \" + formatPrice(data[t]))\n",
    "        \n",
    "        # 이전에 주식을 산 적이 있고(agent.inventory > 0)\n",
    "        # agent의 action==2이면 주식을 팝니다\n",
    "        elif action == 2 and len(agent.inventory) > 0: # sell\n",
    "            # inventory에서 맨 처음 값을 빼옵니다\n",
    "            bought_price = agent.inventory.pop(0)\n",
    "            # 수익이 나면 reward로 수익을 주고\n",
    "            # 손해를 보면 0을 reward로 줍니다\n",
    "            reward = max(data[t] - bought_price, 0)\n",
    "            # total_propit에 수익만큼 더해줍니다\n",
    "            total_profit += data[t] - bought_price\n",
    "            print(\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n",
    "\n",
    "        done = True if t == l - 1 else False\n",
    "        agent.memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(\"--------------------------------\")\n",
    "            print(\"Total Profit: \" + formatPrice(total_profit))\n",
    "            print(\"--------------------------------\")\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.expReplay(batch_size)\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        agent.model.save(\"models/model_ep\" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent의 코드를 알아봅시다\n",
    "# Agent는 Neural Network로 구성되어있습니다\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, is_eval=False, model_name=\"\"):\n",
    "        self.state_size = state_size # normalized previous days\n",
    "        # 3가지 action : hold, buy, sell\n",
    "        self.action_size = 3\n",
    "        # 양방향 큐, 양쪽으로 넣고 뺄수 있습니다\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.inventory = []\n",
    "        self.model_name = model_name\n",
    "        self.is_eval = is_eval\n",
    "        \n",
    "        # hyperparameter 설정\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        self.model = load_model(\"models/\" + model_name) if is_eval else self._model()\n",
    "\n",
    "    def _model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=64, input_dim=self.state_size, activation=\"relu\"))\n",
    "        model.add(Dense(units=32, activation=\"relu\"))\n",
    "        model.add(Dense(units=8, activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    # hold, buy, sell을 결정하는 action\n",
    "    def act(self, state):\n",
    "        if not self.is_eval and random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        options = self.model.predict(state)\n",
    "        return np.argmax(options[0])\n",
    "\n",
    "    def expReplay(self, batch_size):\n",
    "        mini_batch = []\n",
    "        l = len(self.memory)\n",
    "        for i in range(l - batch_size + 1, l):\n",
    "            mini_batch.append(self.memory[i])\n",
    "\n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   \n",
    "#   \n",
    "# CARTPOLE GAME\n",
    "> 최대한 오함마의 중심을 잡아주는 게임입니다\n",
    "\n",
    "![JPEG](https://keon.io/images/deep-q-learning/animation.gif)\n",
    "\n",
    "- STATE는 4개의 정보를 입력받습니다.(오함마의 기울어진 정도, 위치 등등)\n",
    "- AGENT는 0(왼쪽으로 이동), 1(오른쪽으로 이동) 두가지 ACTION을 취합니다\n",
    "#   \n",
    "# DEEP Q NETWORK\n",
    "> NEURAL NETWORK를 통해 우린 한 번도 보지못한 곳의 Q값도 <BR> 좋은 예측값으로 가지고 있어야 합니다\n",
    "\n",
    "####  \n",
    "- 1. 모든 행동들을 위해 예측된 Q값들을 가질 수 있도록 현재상태를 위해 FORWARD 시킵니다\n",
    "- 2. 다음 상태를 위해 통과시키고 결과값의 최댓값을 계산합니다\n",
    "- 3. 최댓값을 향해 행동하는 Q값의 대상을 설정하고, 나머지 행동들은 REWARD를 0으로 합니다\n",
    "- 4. BACKPROP을 통해 WEIGHT를 UPDATE합니다\n",
    "- 5. 이를 통해 현재 STATE에서 UNSEEN INPUT을 가지고 REWARD VALUE를 예측할 수 있습니다\n",
    "\n",
    "![JPEG](https://keon.io/images/deep-q-learning/deep-q-learning.png)\n",
    "#### 초반에는 예측값이 낮아 LOSS가 크지만 차차 예측값이 높아져 LOSS가 작아집니다\n",
    "#   \n",
    "# Q-FUNCTION\n",
    "####   \n",
    "- 먼저 행동 a를 수행하고 보상 r과 새로운 상태 s를 관찰합니다\n",
    "- 결과를 바탕으로 최대 목표 Q를 계산한 후 미래보상이 현재보상보다 낮게 설정합니다\n",
    "- 마지막으로 DISCOUNTED된 미래 보상에 현재보상을 더해 목표값을 얻습니다\n",
    "- 목표값 - 현재 예상치 = LOSS입니다, 이 값을 제곱해 큰 손실값에 더 PENALTY를 줍니다\n",
    "- 또한 음수값을 양수값으로 바꿔줄 수 있습니다\n",
    "![JPEG](https://i.imgur.com/fzIuuTU.png)\n",
    "\n",
    "\n",
    "#   \n",
    "# REMEMBER\n",
    "- NEURAL NETWORK는 이전 기억들을 반영하지않습니다(그래서 RNN이 등장했습니다)\n",
    "- 이를 방지하기 위해 PREVIOUS EXPERIENCE LIST를 만들어 줘야 합니다\n",
    "- MEMORY LIST를 만들고 REMEMBER() 함수를 이용해\n",
    "- STATE, ACTION, REWARD, NEXT STATE를 넣어줍니다\n",
    "\n",
    "![JPEG](https://i.imgur.com/6ljx4Dj.png)\n",
    "![JPEG](https://i.imgur.com/rW7fwWQ.png)\n",
    "#   \n",
    "# REPLAY\n",
    "- REMEMBER를 했으면 이제 REPLAY를 이용해 써먹어야합니다\n",
    "- EXPERIENCE를 이용해 TRAIN 해줍니다\n",
    "- MINIBATCH는 MEMORY LIST에서 랜덤하게 가져와 배치시켜 줍니다\n",
    "\n",
    "![JPEG](https://i.imgur.com/fpv0yVu.png)\n",
    "#   \n",
    "# HYPER PARAMETER\n",
    "> 미래보상을 최대화 하기 위해 HYPER PARAMETER들을 지정해 줍니다\n",
    "\n",
    "- EPISODE - AGENT가 PLAY할 게임의 수\n",
    "- GAMMA - DISCOUNT RATE, 현재 보상을 크게하기 위해 미래 보상을 할인할 비율\n",
    "- EPSILON - 탐색 비율, 처음엔 패턴을 보기전에 AGENT가 모든 종류를 시도하는것이<BR>낫기 때문에 1로 설정하는것이 좋습니다. 이후 EPSILON이 점차 줄면서<BR>\n",
    "    AGENT는 현재 상태를 기반으로 보상값을 예측하고 가장 높은 값을 선택합니다\n",
    "- EPSILON DECAY - 좋은 결과를 위해 차차 EPSILON을 줄여나가야 합니다\n",
    "- EPSILON MIN - 최소한의 EPSILON\n",
    "    \n",
    "    \n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000번 훈련시킵니다\n",
    "EPISODES = 100\n",
    "\n",
    "# DEEP-Q-LEARNING의 AGENT를 정의합니다\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_szie):\n",
    "        self.state_size = state_size\n",
    "        # action_size는 행동의 갯수\n",
    "        self.action_size = action_size\n",
    "        # 메모리를 2000 사이즈의 양방향 큐로 초기화\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        # discount rate\n",
    "        self.gamma = 0.95\n",
    "        # exploration rate, 탐사 속도 ?\n",
    "        # 처음엔 패턴을 보기전에 AGENT가 모든 종류\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    # DEEP-Q LEANRING을 위한 Neural Net을 생성\n",
    "    # Fully Connected Neural Net을 사용하며\n",
    "    # Adam optimizer와 loss function은 Min Squared Error를 사용\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    # state, action, reward, next_state를 저장해둡니다\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # 어떤 action을 취할지 정의합니다\n",
    "    # 우리는 오른쪽으로 이동, 왼쪽으로 이동 2가지만 존재합니다\n",
    "    def act(self, state):\n",
    "        # 처음엔 AGENT가 아무정보가 없기때문에 무작위로 행동하는것이 좋습니다\n",
    "        # 따라서 초기에는 epsilon을 크게 설정하고, 이후 차차 줄여가면서\n",
    "        # 현재 상태의 기반으로 보상값을 예측하고 가장 높은 값을 선택합니다\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        # epsilon이 충분히 낮아지면 Neural Network를 이용해 values를 예측합니다\n",
    "        act_values = self.model.predict(state)\n",
    "        # values중 가장 큰 값을 반환합니다\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    # 컴퓨터 연산을 줄이기위해 replay를 합니다\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            # train이 끝나지 않았다면\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        # 방향성을 유지하기 위해 epsilon을 차차 줄여나갑니다\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/100, score: 17, e: 1.0\n",
      "episode: 1/100, score: 17, e: 0.99\n",
      "episode: 2/100, score: 23, e: 0.88\n",
      "episode: 3/100, score: 16, e: 0.81\n",
      "episode: 4/100, score: 19, e: 0.74\n",
      "episode: 5/100, score: 13, e: 0.69\n",
      "episode: 6/100, score: 16, e: 0.64\n",
      "episode: 7/100, score: 15, e: 0.59\n",
      "episode: 8/100, score: 14, e: 0.55\n",
      "episode: 9/100, score: 14, e: 0.51\n",
      "episode: 10/100, score: 12, e: 0.48\n",
      "episode: 11/100, score: 18, e: 0.44\n",
      "episode: 12/100, score: 7, e: 0.43\n",
      "episode: 13/100, score: 19, e: 0.39\n",
      "episode: 14/100, score: 9, e: 0.37\n",
      "episode: 15/100, score: 9, e: 0.35\n",
      "episode: 16/100, score: 8, e: 0.34\n",
      "episode: 17/100, score: 9, e: 0.33\n",
      "episode: 18/100, score: 8, e: 0.31\n",
      "episode: 19/100, score: 9, e: 0.3\n",
      "episode: 20/100, score: 8, e: 0.29\n",
      "episode: 21/100, score: 9, e: 0.27\n",
      "episode: 22/100, score: 9, e: 0.26\n",
      "episode: 23/100, score: 15, e: 0.24\n",
      "episode: 24/100, score: 18, e: 0.22\n",
      "episode: 25/100, score: 10, e: 0.21\n",
      "episode: 26/100, score: 12, e: 0.2\n",
      "episode: 27/100, score: 7, e: 0.19\n",
      "episode: 28/100, score: 10, e: 0.18\n",
      "episode: 29/100, score: 10, e: 0.17\n",
      "episode: 30/100, score: 9, e: 0.17\n",
      "episode: 31/100, score: 8, e: 0.16\n",
      "episode: 32/100, score: 9, e: 0.15\n",
      "episode: 33/100, score: 9, e: 0.15\n",
      "episode: 34/100, score: 8, e: 0.14\n",
      "episode: 35/100, score: 10, e: 0.13\n",
      "episode: 36/100, score: 9, e: 0.13\n",
      "episode: 37/100, score: 9, e: 0.12\n",
      "episode: 38/100, score: 10, e: 0.12\n",
      "episode: 39/100, score: 8, e: 0.11\n",
      "episode: 40/100, score: 7, e: 0.11\n",
      "episode: 41/100, score: 9, e: 0.1\n",
      "episode: 42/100, score: 8, e: 0.099\n",
      "episode: 43/100, score: 11, e: 0.093\n",
      "episode: 44/100, score: 7, e: 0.09\n",
      "episode: 45/100, score: 17, e: 0.083\n",
      "episode: 46/100, score: 19, e: 0.075\n",
      "episode: 47/100, score: 15, e: 0.07\n",
      "episode: 48/100, score: 12, e: 0.066\n",
      "episode: 49/100, score: 8, e: 0.063\n",
      "episode: 50/100, score: 15, e: 0.059\n",
      "episode: 51/100, score: 11, e: 0.055\n",
      "episode: 52/100, score: 7, e: 0.054\n",
      "episode: 53/100, score: 9, e: 0.051\n",
      "episode: 54/100, score: 9, e: 0.049\n",
      "episode: 55/100, score: 14, e: 0.046\n",
      "episode: 56/100, score: 10, e: 0.043\n",
      "episode: 57/100, score: 11, e: 0.041\n",
      "episode: 58/100, score: 55, e: 0.031\n",
      "episode: 59/100, score: 23, e: 0.028\n",
      "episode: 60/100, score: 23, e: 0.025\n",
      "episode: 61/100, score: 11, e: 0.023\n",
      "episode: 62/100, score: 9, e: 0.022\n",
      "episode: 63/100, score: 17, e: 0.021\n",
      "episode: 64/100, score: 25, e: 0.018\n",
      "episode: 65/100, score: 15, e: 0.017\n",
      "episode: 66/100, score: 15, e: 0.016\n",
      "episode: 67/100, score: 21, e: 0.014\n",
      "episode: 68/100, score: 21, e: 0.013\n",
      "episode: 69/100, score: 16, e: 0.012\n",
      "episode: 70/100, score: 27, e: 0.01\n",
      "episode: 71/100, score: 22, e: 0.01\n",
      "episode: 72/100, score: 14, e: 0.01\n",
      "episode: 73/100, score: 18, e: 0.01\n",
      "episode: 74/100, score: 16, e: 0.01\n",
      "episode: 75/100, score: 32, e: 0.01\n",
      "episode: 76/100, score: 19, e: 0.01\n",
      "episode: 77/100, score: 19, e: 0.01\n",
      "episode: 78/100, score: 35, e: 0.01\n",
      "episode: 79/100, score: 33, e: 0.01\n",
      "episode: 80/100, score: 15, e: 0.01\n",
      "episode: 81/100, score: 23, e: 0.01\n",
      "episode: 82/100, score: 35, e: 0.01\n",
      "episode: 83/100, score: 32, e: 0.01\n",
      "episode: 84/100, score: 22, e: 0.01\n",
      "episode: 85/100, score: 26, e: 0.01\n",
      "episode: 86/100, score: 31, e: 0.01\n",
      "episode: 87/100, score: 37, e: 0.01\n",
      "episode: 88/100, score: 21, e: 0.01\n",
      "episode: 89/100, score: 29, e: 0.01\n",
      "episode: 90/100, score: 36, e: 0.01\n",
      "episode: 91/100, score: 15, e: 0.01\n",
      "episode: 92/100, score: 23, e: 0.01\n",
      "episode: 93/100, score: 28, e: 0.01\n",
      "episode: 94/100, score: 22, e: 0.01\n",
      "episode: 95/100, score: 31, e: 0.01\n",
      "episode: 96/100, score: 28, e: 0.01\n",
      "episode: 97/100, score: 41, e: 0.01\n",
      "episode: 98/100, score: 36, e: 0.01\n",
      "episode: 99/100, score: 35, e: 0.01\n"
     ]
    }
   ],
   "source": [
    "# 훈련을 실행합니다\n",
    "if __name__ == \"__main__\":\n",
    "    # CartPole 게임을 불러와 ENVIRONMENT로 설정합니다\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    # 왼쪽으로 이동, 오른쪽으로 이동 총 2가지\n",
    "    action_size = env.action_space.n\n",
    "    # 우리가 설정한 DQNAgent를 불러옵니다\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    \n",
    "    # 훈련횟수만큼 훈련합니다\n",
    "    for e in range(EPISODES):\n",
    "        # state를 초기화합니다\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        # 한번 훈련마다 500번 탐색합니다\n",
    "        for time in range(500):\n",
    "            # epsilon에 맞게 action을 선택합니다\n",
    "            # epsilon이 높다면 무작위로 선택할것이고(초기에)\n",
    "            # epsilon이 낮다면 방향성에 맞게 action을 취할것입니다\n",
    "            action = agent.act(state)\n",
    "            # action의 결과로 next_state, reward, done 여부를 반환합니다\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # 훈련이 모두 끝나면 reward로 -10을 반환합니다(?)\n",
    "            reward = reward if not done else -10\n",
    "            # 다음 단계를 위해 next_state를 위해 초기화해줍니다\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # 처음부터 다시 계산하지 않기 위해 remember 합니다\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            # 다음단계로 이동\n",
    "            state = next_state\n",
    "            # 훈련이 한번 끝나면 score와 epsilon을 반환\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e+1, EPISODES, time, agent.epsilon))\n",
    "                break\n",
    "            # 예전 experience를 가지고 다시 훈련합니다\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
