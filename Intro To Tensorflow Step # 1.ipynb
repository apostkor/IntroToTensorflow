{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Intro To Tensorflow Step # 1\n",
    "\n",
    "# by PARK-SI HYUNG/Sun-il Kim. 2019-01-20\n",
    "---\n",
    "# # Scikit-Learn\n",
    "#### 캐글에서 많이 보던 모듈입니다\n",
    "- CLASSIFICATION, REGRESSION, CLUSTRING, DIMENSIONALITY REDUCTION 등등\n",
    "- 다양한 라이브러리를 제공합니다\n",
    "- 좋은 PARAMETER를 찾기위한 GRIDSEARCH 등등 사용하기가 쉽고 편리합니다\n",
    "- 하지만 GPU를 사용하는 NEURAL NETWORK는 제공하지 않는 단점이 존재합니다\n",
    "![JPEG](https://cdn-images-1.medium.com/max/1000/1*2NR51X0FDjLB13u4WdYc4g.png)\n",
    "#   \n",
    "# # CAFFE\n",
    "\n",
    "![JPEG](https://i.imgur.com/nDzxPfq.png)\n",
    "\n",
    "#### 2014년에 등장한 딥러닝 프레임워크입니다\n",
    "- 새 LAYER를 설정할 때 마다 교체하는 크기가 매우 커, 유연하지 못한 단점이 있습니다\n",
    "- 작년에 페이스북에서 새로운 포크인 CAFE2를 공개하였습니다\n",
    "#   \n",
    "# # TENSORFLOW\n",
    "\n",
    "![JPEG](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ13eckdxHortqn_W8QTw4gvs1In21gbCyLKvOFfQv_3IzSf9zZ)\n",
    "\n",
    "#### 텐하 - 이젠 너무나 유명해져버린 텐서플로우\n",
    "\n",
    "- 각각의 노드가 TENSOR OPERATION으로 구성됩니다(매트릭스 연산, CONVOLUTION 등등)\n",
    "- 단 몇줄로 모델 구성이 가능합니다 (MODULARITY)\n",
    "- TENSORBOARD의 성능 시각화, 멀티 GPU 지원, 갓 구글의 빵빵한 지원\n",
    "- PLACEHOLDER를 사용해 훈련시키는 동안 데이터를 고정시켜둡니다\n",
    "- 많은 커뮤니티의 참여\n",
    "#   \n",
    "# # KERAS\n",
    "\n",
    "![JPEG](https://cdn-images-1.medium.com/max/652/1*uokXCDa_O7jzfbUC9qS6cg.png)\n",
    "\n",
    "#### 텐서플로우에서 제공하는 HIGH LEVEL API\n",
    "- 추상화가 굉장히 잘 되어 있고 훨씬 직관적인 흐름으로 코드를 작성할 수 있습니다\n",
    "- OBJECT ORIENTED(객체지향)으로 디자인 되어 있습니다\n",
    "\n",
    "#### 객체지향이란 상태와 행위로 이뤄져있습니다.\n",
    "> https://opentutorials.org/course/743/6553 참고\n",
    "#   \n",
    "#   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[-0.2973071   0.79365814]] [0.4199385]\n",
      "20 [[0.01820965 0.3026464 ]] [0.28911197]\n",
      "40 [[0.086591   0.22105087]] [0.29599774]\n",
      "60 [[0.09804291 0.20452267]] [0.2986544]\n",
      "80 [[0.09979719 0.20102811]] [0.2995668]\n",
      "100 [[0.10001093 0.2002485 ]] [0.29986376]\n",
      "120 [[0.10001677 0.20006374]] [0.2999577]\n",
      "140 [[0.10000758 0.20001721]] [0.299987]\n",
      "160 [[0.10000276 0.20000483]] [0.29999602]\n",
      "180 [[0.10000091 0.20000139]] [0.2999988]\n",
      "200 [[0.10000028 0.2000004 ]] [0.29999962]\n"
     ]
    }
   ],
   "source": [
    "# TENSORFLOW 연습을 해봅시다\n",
    "# 탭 버튼을 이용하면 손쉽게 문법이 작성됩니다\n",
    "# 몇줄 안되니 손으로 쳐보고 익혀봅시다 !\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 2X100개의 0~1사이 난수를 생성합니다\n",
    "x_data = np.float32(np.random.rand(2, 100))\n",
    "# [0.100, 0.200]과 x_data를 모두 곱해서 더해주는 dot 연산을 해줍니다\n",
    "# [1X2] X [2X100] => [1X100]의 연산을 수행합니다\n",
    "# 저번주에 배웠던 NEURAL NETWORK에서 사용했었죠 !\n",
    "y_data = np.dot([0.100, 0.200], x_data) + 0.300\n",
    "\n",
    "# Y = WX + b 연산을 하기위한 b, W를 정합니다\n",
    "# b를 0으로 초기화\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "# -1과 0 사이의 1X2의 난수 생성\n",
    "W = tf.Variable(tf.random_uniform([1,2],-1.0, 1.0))\n",
    "# Y = WX + b의 연산을 정의해줍니다\n",
    "y = tf.matmul(W, x_data) + b\n",
    "\n",
    "# loss function을 지정해줍니다. SQUARED ERROR를 사용\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "# 앞에서 배웠던 GradientDescent(경사하강법)을 이용해 손실을 줄여줍니다\n",
    "# 최적화는 손실함수(loss function)을 최소화시카는 파라미터W)들을 찾는 과정입니다\n",
    "# 0.5는 learning rate입니다\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "# optimizer에 loss를 적용시킵니다\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 시작하기 전 모든 변수를 초기화해줍니다\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# TENSORFLOW 연산을 시작하는 세션을 생성합니다\n",
    "sess = tf.Session()\n",
    "# 세션을 초기화합니다\n",
    "sess.run(init)\n",
    "\n",
    "# 200번 세션을 학습시킵니다\n",
    "for step in range(0, 201):\n",
    "    sess.run(train)\n",
    "    # 20번마다 확인하기 위해 print 해줍니다\n",
    "    if step % 20 == 0:\n",
    "        print (step, sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST TUTORIAL\n",
    "\n",
    "- MNIST란 28×28 크기의 0~9사이의 숫자 이미지들로 구성되어있습니다\n",
    "- 머신러닝 연습에 적절하며 6만개의 TRAINING DATA와 1만개의 TEST DATA가 존재합니다\n",
    "\n",
    "![JPEG](http://solarisailab.com/wp-content/uploads/2016/05/MNIST1-e1462350839768.png)\n",
    "\n",
    "\n",
    "#### 여기서 데이터를 받아서 작업하는 노트북 위치에 저장시켜둡시다\n",
    "https://drive.google.com/open?id=1AI963w5hfc7ZCugQ8kMpw3_h9eWPu6GI\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets('./data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습속도를 나타내는 것으로 너무 높으면 수렴하기전에 날아가버릴 수 있고 (오버슈팅)\n",
    "# 너무 낮으면 최적화 하는데 시간이 너무 오래걸릴 수 있습니다(훈련기간 동안 끝나버림)\n",
    "learning_rate = 0.01\n",
    "training_iteration = 30\n",
    "batch_size = 100\n",
    "display_step = 2\n",
    "\n",
    "# 입력받을 데이터를 담는 '그릇'으로 MNIST 이미지가 가로28, 세로28이라\n",
    "# 둘을 곱한 784를 지정해 줍니다\n",
    "# 정답은 0~9 10가지이기 때문에 OUTPUT은 10으로 정해줍니다\n",
    "X = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float', [None, 10])\n",
    "\n",
    "# 초기에 가중치를 랜덤하게 정해줍니다. 계속 변하는 값이기에 Variable을 써줍니다\n",
    "# placeholder는 변하지 않을 값(input, ouput) variable은 변하는 값을 넣어줍니다\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 텐서보드를 사용해봅니다\n",
    "with tf.name_scope('Wx_b') as scope:\n",
    "    # softmax를 이용한 NeuralNetwork를 사용합니다\n",
    "    model = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "    \n",
    "with tf.name_scope('cost_function') as scope:\n",
    "    # 크로스 엔트로피 ?\n",
    "    cost_function = -tf.reduce_sum(y*tf.log(model))\n",
    "    # cost의 변화를 확인하기 위해 변수를 생성합니다\n",
    "    tf.summary.scalar('cost_function', cost_function)\n",
    "with tf.name_scope('train') as scope:\n",
    "    # Gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "\n",
    "# 나중에 가중치를 확인하기 위한 변수를 생성합니다\n",
    "w_h = tf.summary.histogram('weights', W)\n",
    "b_h = tf.summary.histogram('biases', b)\n",
    "\n",
    "# 시작하기전 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0001 cost= 29.860463738\n",
      "Iteration: 0003 cost= 21.028716025\n",
      "Iteration: 0005 cost= 20.172343531\n",
      "Iteration: 0007 cost= 19.677520549\n",
      "Iteration: 0009 cost= 19.444654584\n",
      "Iteration: 0011 cost= 19.047080167\n",
      "Iteration: 0013 cost= 19.021038682\n",
      "Iteration: 0015 cost= 18.713245065\n",
      "Iteration: 0017 cost= 18.658447560\n",
      "Iteration: 0019 cost= 18.516098531\n",
      "Iteration: 0021 cost= 18.513327182\n",
      "Iteration: 0023 cost= 18.396553592\n",
      "Iteration: 0025 cost= 18.367121046\n",
      "Iteration: 0027 cost= 18.132746251\n",
      "Iteration: 0029 cost= 18.126241561\n",
      "Tuning completed!\n",
      "Accuracy: 0.9227\n"
     ]
    }
   ],
   "source": [
    "# 세션을 시작합니다\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # summary들을 저장할 경로를 지정합니다\n",
    "    summary_writer = tf.summary.FileWriter('./data/logs', graph=sess.graph)\n",
    "    \n",
    "    # 위에서 지정한 횟수만큼 훈련해줍니다\n",
    "    for iteration in range(training_iteration):\n",
    "        # cost를 0으로 초기화\n",
    "        avg_cost = 0\n",
    "        # mnist에서 지정한 배치사이즈(한번에 몇칸을 읽을지)로 지정\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # 위에서 지정한 optimizer(GradientDescentOptimizer)를 사용\n",
    "            # input인 x, 정답(label)인 y를 feed_dict에 넣어줍니다 \n",
    "            sess.run(optimizer, feed_dict={X:batch_xs, y:batch_ys})\n",
    "            # loss를 모두 더하고 나중에 평균으로 계산합니다\n",
    "            avg_cost += sess.run(cost_function, feed_dict={X:batch_xs, y:batch_ys})/total_batch\n",
    "            # 매 횟수(epoch라고도 부릅니다)마다 상태를 기록\n",
    "            summary_str = sess.run(merged_summary_op, feed_dict={X: batch_xs, y: batch_ys})\n",
    "            summary_writer.add_summary(summary_str, iteration*total_batch + i)\n",
    "        if iteration % display_step == 0:\n",
    "            print(\"Iteration:\", '%04d' % (iteration + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "        \n",
    "    print('Tuning completed!')\n",
    "    \n",
    "    \n",
    "    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    # 모델의 정확도를 계산하여 출력합니다. \n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, y: mnist.test.labels}))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   \n",
    "#   \n",
    "#   \n",
    "# PREPARE A DATASET\n",
    "> 훈련에 사용하는 데이터를 준비하는 과정을 알아봅시다\n",
    "#   \n",
    "# 1. SELECT THE DATA\n",
    "> 우리는 상황에 맞는, 알맞은 데이터를 선택해야 합니다\n",
    "\n",
    "- 예측하고 싶은 모델을 만들기 위해선, 예측에 알맞는 데이터를 선택해야합니다\n",
    "- 숫자를 인식하는데 알파벳 문자를 입력으로 넣으면 안되듯이\n",
    "- 우리가 필요로 하는 데이터를 제공하는 사이트가 많습니다. (EX: KAGGLE)\n",
    "- 주로 CSV(데이터들을 쉼표로 구분한 텍스트 파일)을 사용합니다\n",
    "![JPEG](https://upload.wikimedia.org/wikipedia/commons/6/6d/Data_types_-_en.svg)\n",
    "\n",
    "\n",
    "# 2. PROCESS THE DATA\n",
    "> 데이터 분석의 끝판왕, 데이터를 얻었으면 예측하기 쉽게 잘 가공해줘야 합니다\n",
    "\n",
    "- EDA(EXPLORATORY DATA ANALYSIS)라고도 합니다.\n",
    "- 맨처음으로는 DATA SET을 확인합니다. 비만도를 예측한다면 CM, KG등을 사용하겠죠.\n",
    "- 다음으로는 결측치 처리를 해줍니다. 10000KG이나 30CM같은 이상한 데이터를 처리해줍니다\n",
    "- 마지막으로 우리의 입맛에 맞게 FEATURE ENGINEERING을 해줍니다.\n",
    "- 부모의 수 + 형제자매 수를 더해 FAMILY SIZE를 만들수도 있고 다양하게 만들어봅니다.\n",
    "\n",
    "![JPEG](https://i.imgur.com/ZbxRDBF.png)\n",
    "\n",
    "# 3. TRANSFORM THE DATA\n",
    "> DECOMPOSE(분해) / VECTOR화\n",
    "\n",
    "- 2017-4-10같은 날짜형식의 데이터에서 월만 사용하고 싶다면 4만 빼옵니다\n",
    "- PROCESS한 DATA를 벡터화 시킵니다. 컴퓨터는 숫자로만 인식하기 때문에\n",
    "- CHILD : 0, ADULT : 1같은 형식으로 벡터화 시킵니다. (ONE-HOT ENCODING)\n",
    "\n",
    "![JPEG](https://cdn-images-1.medium.com/max/1200/1*YEJf9BQQh0ma1ECs6x_7yQ.png)\n",
    "#   \n",
    "\n",
    "# GENERATE MUSIC IN TENSORFLOW\n",
    "> 텐서플로우로 음악을 만들어 봅시다\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# WAVENET\n",
    "###   \n",
    "- DEEPMIND에서 개발한 TTS(TEXT TO SPEACH, 텍스트를 음성으로 변환)\n",
    "- 심층 신경망(DEEP NEURAL NETWORK, DNN)을 사용합니다\n",
    "\n",
    "![JPEG](https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig1-Anim-160908-r01.gif)\n",
    "#   \n",
    "# DNN(DEEP NEURAL NETWORK)란 ?\n",
    "###   \n",
    "- 입력층(INPUT)과 출력층(OUPUT) 사이에 여러 개의 은닉층(HIDDEN LAYER)로 구성\n",
    "- 추가 LAYER들은 점진적으로 모여진 하위 LAYER의 특징들을 잘 잡아낼 수 있습니다\n",
    "- 일반적인 신경망에 비해 더 적은 수의 유닛(NODE)으로 구현 가능합니다\n",
    "\n",
    "![JPEG](https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif)\n",
    "#   \n",
    "# HOW IT WORKS ?\n",
    "###    \n",
    "![JPEG](https://i.imgur.com/D7F4GFB.png)\n",
    "\n",
    "- INPUT DATA(SINGLE NODE)가 정제되지 않은 소리 파일로 시작합니다\n",
    "- 전처리 과정에서 소리를 파형으로 바꿔줘, 정제하기 쉽게 만듭니다\n",
    "- TENSOR를 만들기 위해 파형을 숫자로 바꿔주는 ONE HOT ENCODING 작업을 합니다\n",
    "- CONVOLUTIONAL NETWORK의 첫번째 층에 넣어줍니다(이 과정에서 필요없는 채널을 삭제)\n",
    "- 여러 층을 거친 후 OUTPUT을 모두 결합해, 고차원의 음성으로 만들어줍니다\n",
    "- 훈련이 잘 이뤄지고 있는지 LOSS FUNCTION을 거칩니다\n",
    "- 다음 단계의 오디오(다음 가사) 생성을 위해, 생성된 OUTPUT이 다시 INPUT으로 들어갑니다\n",
    "- 이런 단계를 반복해줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2년전 코드라 작동이 잘 되지않습니다 ㅜ\n",
    "# 코드만 한번 훑어봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'midi_manipulation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-8cdcb34a374e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmidi_manipulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# hyperparameter 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'midi_manipulation'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import midi_manipulation\n",
    "\n",
    "# hyperparameter 설정\n",
    "# restricted Boltzmann machine이란 Neural Network를 사용합니다\n",
    " \n",
    "# 가장 높은음과 가장 낮은음을 측정합니다\n",
    "lowest_note = midi_manipulation.lowerBound\n",
    "highest_note = midi_manipulation.upperBound\n",
    "note_range = highest_node - lowest_note\n",
    "\n",
    "# 한번에 몇초를 만들지 결정\n",
    "num_timestpes = 15\n",
    "# size of the visible layer\n",
    "n_visible = 2*note_range_*num_timesteps\n",
    "# size of the hidden layer\n",
    "n_hidden = 50\n",
    "# 훈련시킬 횟수\n",
    "num_epochs = 200\n",
    "# 한번에 훈련시킬 사이즈\n",
    "batch_size = 100\n",
    "# learning_rate\n",
    "lr = tf.constant(0.005, tf.float32)\n",
    "\n",
    "# 데이터를 담아줄 placeholder\n",
    "x = tf.placeholder(tf.float32, [None, n_visible], name='x')\n",
    "W = tf.Variable(tf.random_normal([n_visible, n_hidden], 0.01), name='W')\n",
    "bh = tf.Variable(tf.zeros([1, n_hidden], tf.float32, name='bh'))\n",
    "bv = tf.Variable(tf.zeros([1, n_visible], tf.float32, name='bv'))\n",
    "\n",
    "# 샘플을 생성합니다\n",
    "# gibbs_smaple은 두 개 이상의 확률변수의 결합확률분포로부터 일련의 표본을 생성합니다\n",
    "# 이전 데이터에 얼마나 의존하는지 랜덤하게 생성\n",
    "x_sample = gibbs_sample(1)\n",
    "# hidden node의 샘플, x의 visible state로부터 얻음\n",
    "h = sample(tf.sigmoid(tf.matmul(x, W) + bh))\n",
    "# hidden node의 샘플, x_sample의 visible state로부터 얻음\n",
    "h_sample = sample(tf.sigmoid(tf.matmul(x_sample, W) + bh))\n",
    "\n",
    "# 샘플간의 차이를 통해 W, bh, bv를 업데이트 해줍니다\n",
    "\n",
    "# tf.cast는 int형식을 float형식으로 바꾸는데 사용합니다\n",
    "# x의 사이즈를 불러옵니다\n",
    "size_bt = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "W_adder = tf.mul(lr/size_bt, tf.sub(tf.matmul(tf.transpose(x), h),\n",
    "                 tf.matmul(tf.transpose(x_sample), h_sample)))\n",
    "bv_adder = tf.mul(lr/size_bt, tf.reduce_sum(tf.sub(x, x_sample), 0, True))\n",
    "bh_adder = tf.mul(lr/size_bt, tf.reduce_sum())\n",
    "\n",
    "# 훈련 세션을 시작합니다\n",
    "with tf.Session() as sess:\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    # 지정한 훈련횟수 만큼 훈련해줍니다\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # 노래들을 훈련하기 쉽게 벡터화해줍니다\n",
    "        for song in songs:\n",
    "            song = np.array(song)\n",
    "            for i in range(1, len(song), batch_size):\n",
    "                tr_x = song[i:i+batch_size]\n",
    "                sess.run(updt, feed_dict={x: tr_x})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    \n",
    "#    \n",
    "#   \n",
    "# HOW TO MAKE DATA AMAZING\n",
    "#   \n",
    "![JPEG](http://successflow.co.uk/wp-content/uploads/2015/11/Data-is-the-new-oil-header.jpg)\n",
    "#   \n",
    "- 데이터는 사전적 정의로 사실을 나타내는 수치라고 합니다\n",
    "- 현실에서 가공되지 않은 상태로 다양한 형태로 존재하고 있습니다(기온, 습도, 주가 등등)\n",
    "- 우리는 가공되지 않은 데이터를 가지고 패턴과 연결점을 찾는 일을 합니다\n",
    "- 그런만큼 우리는 훈련시킬 데이터 선택을 잘해야 합니다(똥을 넣으면 똥이나옵니다 !)\n",
    "---\n",
    "#### DATASET은 CSV(COMMA SEPERATED VALUES)파일을 주로 사용합니다\n",
    "> 일반 텍스트로 저장되기 때문에 편리합니다\n",
    "\n",
    "#   \n",
    "![JPEG](https://i1.wp.com/blog.onestreamsoftware.com/hs-fs/hubfs/blog%20garbage%201.png?w=600&ssl=1)\n",
    "#   \n",
    "# 1. CLEANING\n",
    "- 데이터에 결측치(너무 높거나 낮아서 영향을 주는 데이터)가 있는지 확인합니다\n",
    "- 이런 결측치들을 데이터의 평균으로 대체해주거나, 없애버립니다\n",
    "- 혹은 비어있는 데이터에 값을 입력해줍니다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attr5_3     6362\n",
       "sinc5_3     6362\n",
       "intel5_3    6362\n",
       "fun5_3      6362\n",
       "amb5_3      6362\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Speed Dating Data.csv', encoding='ISO-8859-1')\n",
    "# isnull 기능을 이용해 결측치가 얼마나 있는지 확인해봅니다\n",
    "# null값은 1을 반환하므로 sum으로 총합을 더하면 몇개나 있는지 알 수 있습니다\n",
    "# 너무많아서 뒤에 5개만 확인해봅니다\n",
    "data.isnull().sum().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TRANSFORMATION\n",
    "- WOMEN, FEMALE같은 중복된 데이터가 있으면 통합해줍니다\n",
    "- 데이터의 단위를 통합해주는 NOMARLIZATION을 합니다 (Scikit learn의 StandardScaler)\n",
    "- 예를들어 키와 몸무게는 단위가 다르니 통합해줘야합니다\n",
    "\n",
    "# 3. REDUCTION\n",
    "- 고차원의 데이터(사진, 영상, 음성)등은 1차원으로 바꿔줘야 훈련하기가 좋습니다\n",
    "- 1차원으로 데이터를 바꿔줘 벡터화 시켜줍니다.\n",
    "- REDUCTION의 한가지 종류로 PCA가 있습니다\n",
    "\n",
    "![JPEG](https://i.imgur.com/igUpGIf.png)\n",
    "\n",
    "# PCA(PRINCIPAL COMPONENT ANALYSIS)\n",
    "> 데이터 주성분 분석\n",
    "### Step 1. Normalize Data\n",
    "\n",
    "### Step 2. Compute Covariance Matrix(공분산)\n",
    "- 데이터간의 연관성이 있는지 상관관계를 분석합니다\n",
    "\n",
    "### Step 3. Eigen Decomposition(고유값 분해)\n",
    "- 어떤 벡터를 행렬에 곱한 결과로 나타난 벡터가 원래의 벡터와 같은 방향을 가지면 <br>고유벡터라 합니다\n",
    "- 고유값과 고유벡터를 찾는 작업을 고유값 분해라고 합니다\n",
    "- https://wikidocs.net/7646, https://datascienceschool.net/view-notebook/7fd58178d9e64be682058db7e024d8b5/<br> 참고해서 읽어봅시다\n",
    "### Step 4. Create Projection Matrix\n",
    "- 내림차순으로 정렬한 후, 벡터에 넣어줍니다\n",
    "### Step 5. Squash Features with Projection Matrix\n",
    "- 정렬한 벡터를 numpy의 dot product를 이용해 원래의 데이터와 결합합니다\n",
    "- 이후 2차원 평면에 시각화 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apostcto/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/apostcto/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.78378062, -1.        , -1.72071915, -0.44438879],\n",
       "       [-1.78378062, -1.        , -1.714413  , -0.44438879],\n",
       "       [-1.78378062, -1.        , -1.70810685,  2.25028179],\n",
       "       ...,\n",
       "       [ 1.69090663,  1.        ,  1.53955909, -0.44438879],\n",
       "       [ 1.69090663,  1.        ,  1.54586523, -0.44438879],\n",
       "       [ 1.69090663,  1.        ,  1.55217138, -0.44438879]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nomarlize를 위해 몇가지 변수만 가지고 연습을 해보겠습니다\n",
    "# 4개 int,float 변수만 고릅니다\n",
    "data = data[['iid', 'gender', 'pid', 'match']]\n",
    "# PCA를 위해 결측치를 처리해줍니다\n",
    "data = data.dropna()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaled_X_dummied = StandardScaler().fit_transform(data)\n",
    "scaled_X_dummied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.46704989, -1.00052472],\n",
       "       [ 2.46259197, -1.00085961],\n",
       "       [ 2.52080536, -1.00119451],\n",
       "       ...,\n",
       "       [-2.2940017 ,  1.00521321],\n",
       "       [-2.29845962,  1.00487832],\n",
       "       [-2.30291753,  1.00454343]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale된 x를 가지고 pca를 해봅니다\n",
    "pca = PCA(n_components = 2)\n",
    "x_pca = pca.fit_transform(scaled_X_dummied)\n",
    "x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
